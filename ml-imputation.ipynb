{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Value Imputation using ML Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>530101</td>\n",
       "      <td>38.5</td>\n",
       "      <td>66.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>8.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>11300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>534817</td>\n",
       "      <td>39.2</td>\n",
       "      <td>88.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>530334</td>\n",
       "      <td>38.3</td>\n",
       "      <td>40.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>5290409</td>\n",
       "      <td>39.1</td>\n",
       "      <td>164.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>48.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>530255</td>\n",
       "      <td>37.3</td>\n",
       "      <td>104.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>74.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>533886</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>55.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>527702</td>\n",
       "      <td>37.2</td>\n",
       "      <td>72.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>529386</td>\n",
       "      <td>37.5</td>\n",
       "      <td>72.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>530612</td>\n",
       "      <td>36.5</td>\n",
       "      <td>100.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>534618</td>\n",
       "      <td>37.2</td>\n",
       "      <td>40.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>36.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6112</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0   1        2     3      4     5    6    7    8    9   ...    18    19  \\\n",
       "0    2.0   1   530101  38.5   66.0  28.0  3.0  3.0  NaN  2.0  ...  45.0   8.4   \n",
       "1    1.0   1   534817  39.2   88.0  20.0  NaN  NaN  4.0  1.0  ...  50.0  85.0   \n",
       "2    2.0   1   530334  38.3   40.0  24.0  1.0  1.0  3.0  1.0  ...  33.0   6.7   \n",
       "3    1.0   9  5290409  39.1  164.0  84.0  4.0  1.0  6.0  2.0  ...  48.0   7.2   \n",
       "4    2.0   1   530255  37.3  104.0  35.0  NaN  NaN  6.0  2.0  ...  74.0   7.4   \n",
       "..   ...  ..      ...   ...    ...   ...  ...  ...  ...  ...  ...   ...   ...   \n",
       "295  1.0   1   533886   NaN  120.0  70.0  4.0  NaN  4.0  2.0  ...  55.0  65.0   \n",
       "296  2.0   1   527702  37.2   72.0  24.0  3.0  2.0  4.0  2.0  ...  44.0   NaN   \n",
       "297  1.0   1   529386  37.5   72.0  30.0  4.0  3.0  4.0  1.0  ...  60.0   6.8   \n",
       "298  1.0   1   530612  36.5  100.0  24.0  3.0  3.0  3.0  1.0  ...  50.0   6.0   \n",
       "299  1.0   1   534618  37.2   40.0  20.0  NaN  NaN  NaN  NaN  ...  36.0  62.0   \n",
       "\n",
       "      20   21   22  23     24  25  26  27  \n",
       "0    NaN  NaN  2.0   2  11300   0   0   2  \n",
       "1    2.0  2.0  3.0   2   2208   0   0   2  \n",
       "2    NaN  NaN  1.0   2      0   0   0   1  \n",
       "3    3.0  5.3  2.0   1   2208   0   0   1  \n",
       "4    NaN  NaN  2.0   2   4300   0   0   2  \n",
       "..   ...  ...  ...  ..    ...  ..  ..  ..  \n",
       "295  NaN  NaN  3.0   2   3205   0   0   2  \n",
       "296  3.0  3.3  3.0   1   2208   0   0   1  \n",
       "297  NaN  NaN  2.0   1   3205   0   0   2  \n",
       "298  3.0  3.4  1.0   1   2208   0   0   1  \n",
       "299  1.0  1.0  3.0   2   6112   0   0   2  \n",
       "\n",
       "[300 rows x 28 columns]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The horse colic dataset describes medical characteristics of horses with colic and whether they lived or died.\n",
    "# There are 300 rows and 26 input variables with one output variable. It is a binary classification prediction task that involves predicting 1 if the horse lived and 2 if the horse died.\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv'\n",
    "ds = pd.read_csv(url, header=None, na_values='?')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.003333\n",
       "1     0.000000\n",
       "2     0.000000\n",
       "3     0.200000\n",
       "4     0.080000\n",
       "5     0.193333\n",
       "6     0.186667\n",
       "7     0.230000\n",
       "8     0.156667\n",
       "9     0.106667\n",
       "10    0.183333\n",
       "11    0.146667\n",
       "12    0.186667\n",
       "13    0.346667\n",
       "14    0.353333\n",
       "15    0.823333\n",
       "16    0.340000\n",
       "17    0.393333\n",
       "18    0.096667\n",
       "19    0.110000\n",
       "20    0.550000\n",
       "21    0.660000\n",
       "22    0.003333\n",
       "23    0.000000\n",
       "24    0.000000\n",
       "25    0.000000\n",
       "26    0.000000\n",
       "27    0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = ds.iloc[:, :-1], ds.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19814814814814813"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isna().mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    MLPClassifier(max_iter=1000, verbose=True, random_state=42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.72612800\n",
      "Iteration 2, loss = 0.70327473\n",
      "Iteration 3, loss = 0.68370743\n",
      "Iteration 4, loss = 0.66613241\n",
      "Iteration 5, loss = 0.65132236\n",
      "Iteration 6, loss = 0.63683722\n",
      "Iteration 7, loss = 0.62396852\n",
      "Iteration 8, loss = 0.61261327\n",
      "Iteration 9, loss = 0.60116629\n",
      "Iteration 10, loss = 0.59093314\n",
      "Iteration 11, loss = 0.58129814\n",
      "Iteration 12, loss = 0.57180421\n",
      "Iteration 13, loss = 0.56369980\n",
      "Iteration 14, loss = 0.55575213\n",
      "Iteration 15, loss = 0.54747788\n",
      "Iteration 16, loss = 0.54028939\n",
      "Iteration 17, loss = 0.53280124\n",
      "Iteration 18, loss = 0.52590428\n",
      "Iteration 19, loss = 0.51935600\n",
      "Iteration 20, loss = 0.51289011\n",
      "Iteration 21, loss = 0.50671837\n",
      "Iteration 22, loss = 0.50087510\n",
      "Iteration 23, loss = 0.49516291\n",
      "Iteration 24, loss = 0.48949828\n",
      "Iteration 25, loss = 0.48454879\n",
      "Iteration 26, loss = 0.47907500\n",
      "Iteration 27, loss = 0.47406306\n",
      "Iteration 28, loss = 0.46916781\n",
      "Iteration 29, loss = 0.46430583\n",
      "Iteration 30, loss = 0.45970468\n",
      "Iteration 31, loss = 0.45499709\n",
      "Iteration 32, loss = 0.45044878\n",
      "Iteration 33, loss = 0.44599419\n",
      "Iteration 34, loss = 0.44224901\n",
      "Iteration 35, loss = 0.43778603\n",
      "Iteration 36, loss = 0.43387409\n",
      "Iteration 37, loss = 0.42993098\n",
      "Iteration 38, loss = 0.42637997\n",
      "Iteration 39, loss = 0.42261769\n",
      "Iteration 40, loss = 0.41879897\n",
      "Iteration 41, loss = 0.41538808\n",
      "Iteration 42, loss = 0.41144346\n",
      "Iteration 43, loss = 0.40778710\n",
      "Iteration 44, loss = 0.40428611\n",
      "Iteration 45, loss = 0.40062711\n",
      "Iteration 46, loss = 0.39726993\n",
      "Iteration 47, loss = 0.39392301\n",
      "Iteration 48, loss = 0.39101581\n",
      "Iteration 49, loss = 0.38799764\n",
      "Iteration 50, loss = 0.38489717\n",
      "Iteration 51, loss = 0.38186211\n",
      "Iteration 52, loss = 0.37881825\n",
      "Iteration 53, loss = 0.37578098\n",
      "Iteration 54, loss = 0.37283512\n",
      "Iteration 55, loss = 0.36987489\n",
      "Iteration 56, loss = 0.36704424\n",
      "Iteration 57, loss = 0.36422120\n",
      "Iteration 58, loss = 0.36138799\n",
      "Iteration 59, loss = 0.35850698\n",
      "Iteration 60, loss = 0.35582042\n",
      "Iteration 61, loss = 0.35318119\n",
      "Iteration 62, loss = 0.35084444\n",
      "Iteration 63, loss = 0.34819781\n",
      "Iteration 64, loss = 0.34569334\n",
      "Iteration 65, loss = 0.34318208\n",
      "Iteration 66, loss = 0.34088311\n",
      "Iteration 67, loss = 0.33837117\n",
      "Iteration 68, loss = 0.33573723\n",
      "Iteration 69, loss = 0.33327593\n",
      "Iteration 70, loss = 0.33099629\n",
      "Iteration 71, loss = 0.32878823\n",
      "Iteration 72, loss = 0.32684804\n",
      "Iteration 73, loss = 0.32469114\n",
      "Iteration 74, loss = 0.32233576\n",
      "Iteration 75, loss = 0.32005092\n",
      "Iteration 76, loss = 0.31762191\n",
      "Iteration 77, loss = 0.31535927\n",
      "Iteration 78, loss = 0.31301080\n",
      "Iteration 79, loss = 0.31065382\n",
      "Iteration 80, loss = 0.30825627\n",
      "Iteration 81, loss = 0.30581787\n",
      "Iteration 82, loss = 0.30340243\n",
      "Iteration 83, loss = 0.30124032\n",
      "Iteration 84, loss = 0.29881597\n",
      "Iteration 85, loss = 0.29656004\n",
      "Iteration 86, loss = 0.29439059\n",
      "Iteration 87, loss = 0.29226598\n",
      "Iteration 88, loss = 0.29048987\n",
      "Iteration 89, loss = 0.28826506\n",
      "Iteration 90, loss = 0.28622675\n",
      "Iteration 91, loss = 0.28426081\n",
      "Iteration 92, loss = 0.28231073\n",
      "Iteration 93, loss = 0.28024063\n",
      "Iteration 94, loss = 0.27810585\n",
      "Iteration 95, loss = 0.27620789\n",
      "Iteration 96, loss = 0.27433178\n",
      "Iteration 97, loss = 0.27247647\n",
      "Iteration 98, loss = 0.27064206\n",
      "Iteration 99, loss = 0.26859052\n",
      "Iteration 100, loss = 0.26665674\n",
      "Iteration 101, loss = 0.26437009\n",
      "Iteration 102, loss = 0.26251007\n",
      "Iteration 103, loss = 0.26066671\n",
      "Iteration 104, loss = 0.25893073\n",
      "Iteration 105, loss = 0.25705863\n",
      "Iteration 106, loss = 0.25524244\n",
      "Iteration 107, loss = 0.25341858\n",
      "Iteration 108, loss = 0.25164201\n",
      "Iteration 109, loss = 0.24972459\n",
      "Iteration 110, loss = 0.24783073\n",
      "Iteration 111, loss = 0.24596020\n",
      "Iteration 112, loss = 0.24421425\n",
      "Iteration 113, loss = 0.24242042\n",
      "Iteration 114, loss = 0.24078701\n",
      "Iteration 115, loss = 0.23915680\n",
      "Iteration 116, loss = 0.23732415\n",
      "Iteration 117, loss = 0.23563243\n",
      "Iteration 118, loss = 0.23403677\n",
      "Iteration 119, loss = 0.23235711\n",
      "Iteration 120, loss = 0.23100996\n",
      "Iteration 121, loss = 0.22932000\n",
      "Iteration 122, loss = 0.22775155\n",
      "Iteration 123, loss = 0.22603518\n",
      "Iteration 124, loss = 0.22446313\n",
      "Iteration 125, loss = 0.22284934\n",
      "Iteration 126, loss = 0.22119160\n",
      "Iteration 127, loss = 0.21963340\n",
      "Iteration 128, loss = 0.21802367\n",
      "Iteration 129, loss = 0.21653400\n",
      "Iteration 130, loss = 0.21501572\n",
      "Iteration 131, loss = 0.21360941\n",
      "Iteration 132, loss = 0.21225389\n",
      "Iteration 133, loss = 0.21068776\n",
      "Iteration 134, loss = 0.20924560\n",
      "Iteration 135, loss = 0.20771692\n",
      "Iteration 136, loss = 0.20604144\n",
      "Iteration 137, loss = 0.20463061\n",
      "Iteration 138, loss = 0.20309609\n",
      "Iteration 139, loss = 0.20173144\n",
      "Iteration 140, loss = 0.20023123\n",
      "Iteration 141, loss = 0.19878314\n",
      "Iteration 142, loss = 0.19730001\n",
      "Iteration 143, loss = 0.19602579\n",
      "Iteration 144, loss = 0.19457693\n",
      "Iteration 145, loss = 0.19320077\n",
      "Iteration 146, loss = 0.19173594\n",
      "Iteration 147, loss = 0.19040604\n",
      "Iteration 148, loss = 0.18901265\n",
      "Iteration 149, loss = 0.18748417\n",
      "Iteration 150, loss = 0.18605299\n",
      "Iteration 151, loss = 0.18453916\n",
      "Iteration 152, loss = 0.18321767\n",
      "Iteration 153, loss = 0.18184413\n",
      "Iteration 154, loss = 0.18049282\n",
      "Iteration 155, loss = 0.17920093\n",
      "Iteration 156, loss = 0.17791596\n",
      "Iteration 157, loss = 0.17685360\n",
      "Iteration 158, loss = 0.17571555\n",
      "Iteration 159, loss = 0.17465666\n",
      "Iteration 160, loss = 0.17338783\n",
      "Iteration 161, loss = 0.17215022\n",
      "Iteration 162, loss = 0.17088890\n",
      "Iteration 163, loss = 0.16963288\n",
      "Iteration 164, loss = 0.16822989\n",
      "Iteration 165, loss = 0.16698333\n",
      "Iteration 166, loss = 0.16555704\n",
      "Iteration 167, loss = 0.16450305\n",
      "Iteration 168, loss = 0.16348327\n",
      "Iteration 169, loss = 0.16264712\n",
      "Iteration 170, loss = 0.16151536\n",
      "Iteration 171, loss = 0.16022096\n",
      "Iteration 172, loss = 0.15914057\n",
      "Iteration 173, loss = 0.15785116\n",
      "Iteration 174, loss = 0.15669678\n",
      "Iteration 175, loss = 0.15591106\n",
      "Iteration 176, loss = 0.15466653\n",
      "Iteration 177, loss = 0.15359451\n",
      "Iteration 178, loss = 0.15265971\n",
      "Iteration 179, loss = 0.15157750\n",
      "Iteration 180, loss = 0.15058066\n",
      "Iteration 181, loss = 0.14961978\n",
      "Iteration 182, loss = 0.14878217\n",
      "Iteration 183, loss = 0.14783429\n",
      "Iteration 184, loss = 0.14678839\n",
      "Iteration 185, loss = 0.14576870\n",
      "Iteration 186, loss = 0.14462202\n",
      "Iteration 187, loss = 0.14376795\n",
      "Iteration 188, loss = 0.14282988\n",
      "Iteration 189, loss = 0.14198017\n",
      "Iteration 190, loss = 0.14112761\n",
      "Iteration 191, loss = 0.14013620\n",
      "Iteration 192, loss = 0.13937036\n",
      "Iteration 193, loss = 0.13853891\n",
      "Iteration 194, loss = 0.13790795\n",
      "Iteration 195, loss = 0.13704725\n",
      "Iteration 196, loss = 0.13613459\n",
      "Iteration 197, loss = 0.13496292\n",
      "Iteration 198, loss = 0.13351271\n",
      "Iteration 199, loss = 0.13237398\n",
      "Iteration 200, loss = 0.13145052\n",
      "Iteration 201, loss = 0.13071383\n",
      "Iteration 202, loss = 0.12983798\n",
      "Iteration 203, loss = 0.12901476\n",
      "Iteration 204, loss = 0.12824183\n",
      "Iteration 205, loss = 0.12730662\n",
      "Iteration 206, loss = 0.12642334\n",
      "Iteration 207, loss = 0.12555539\n",
      "Iteration 208, loss = 0.12473978\n",
      "Iteration 209, loss = 0.12391510\n",
      "Iteration 210, loss = 0.12307047\n",
      "Iteration 211, loss = 0.12208072\n",
      "Iteration 212, loss = 0.12130528\n",
      "Iteration 213, loss = 0.12052933\n",
      "Iteration 214, loss = 0.11988934\n",
      "Iteration 215, loss = 0.11905044\n",
      "Iteration 216, loss = 0.11836629\n",
      "Iteration 217, loss = 0.11762403\n",
      "Iteration 218, loss = 0.11676688\n",
      "Iteration 219, loss = 0.11588569\n",
      "Iteration 220, loss = 0.11521070\n",
      "Iteration 221, loss = 0.11442985\n",
      "Iteration 222, loss = 0.11372193\n",
      "Iteration 223, loss = 0.11295385\n",
      "Iteration 224, loss = 0.11233272\n",
      "Iteration 225, loss = 0.11178347\n",
      "Iteration 226, loss = 0.11123704\n",
      "Iteration 227, loss = 0.11046419\n",
      "Iteration 228, loss = 0.10972010\n",
      "Iteration 229, loss = 0.10893046\n",
      "Iteration 230, loss = 0.10821482\n",
      "Iteration 231, loss = 0.10759623\n",
      "Iteration 232, loss = 0.10700996\n",
      "Iteration 233, loss = 0.10621033\n",
      "Iteration 234, loss = 0.10545876\n",
      "Iteration 235, loss = 0.10467132\n",
      "Iteration 236, loss = 0.10397682\n",
      "Iteration 237, loss = 0.10318267\n",
      "Iteration 238, loss = 0.10262052\n",
      "Iteration 239, loss = 0.10185114\n",
      "Iteration 240, loss = 0.10121243\n",
      "Iteration 241, loss = 0.10050396\n",
      "Iteration 242, loss = 0.09978617\n",
      "Iteration 243, loss = 0.09925929\n",
      "Iteration 244, loss = 0.09878144\n",
      "Iteration 245, loss = 0.09811872\n",
      "Iteration 246, loss = 0.09739190\n",
      "Iteration 247, loss = 0.09680780\n",
      "Iteration 248, loss = 0.09577201\n",
      "Iteration 249, loss = 0.09523638\n",
      "Iteration 250, loss = 0.09464497\n",
      "Iteration 251, loss = 0.09399799\n",
      "Iteration 252, loss = 0.09336560\n",
      "Iteration 253, loss = 0.09283715\n",
      "Iteration 254, loss = 0.09229793\n",
      "Iteration 255, loss = 0.09188458\n",
      "Iteration 256, loss = 0.09132223\n",
      "Iteration 257, loss = 0.09070436\n",
      "Iteration 258, loss = 0.08999276\n",
      "Iteration 259, loss = 0.08926979\n",
      "Iteration 260, loss = 0.08871649\n",
      "Iteration 261, loss = 0.08818101\n",
      "Iteration 262, loss = 0.08759017\n",
      "Iteration 263, loss = 0.08698535\n",
      "Iteration 264, loss = 0.08639605\n",
      "Iteration 265, loss = 0.08582106\n",
      "Iteration 266, loss = 0.08520128\n",
      "Iteration 267, loss = 0.08470967\n",
      "Iteration 268, loss = 0.08419049\n",
      "Iteration 269, loss = 0.08373163\n",
      "Iteration 270, loss = 0.08325731\n",
      "Iteration 271, loss = 0.08285356\n",
      "Iteration 272, loss = 0.08231223\n",
      "Iteration 273, loss = 0.08176999\n",
      "Iteration 274, loss = 0.08112800\n",
      "Iteration 275, loss = 0.08075904\n",
      "Iteration 276, loss = 0.08016666\n",
      "Iteration 277, loss = 0.07963109\n",
      "Iteration 278, loss = 0.07914748\n",
      "Iteration 279, loss = 0.07867155\n",
      "Iteration 280, loss = 0.07831803\n",
      "Iteration 281, loss = 0.07788560\n",
      "Iteration 282, loss = 0.07739271\n",
      "Iteration 283, loss = 0.07686977\n",
      "Iteration 284, loss = 0.07636502\n",
      "Iteration 285, loss = 0.07589263\n",
      "Iteration 286, loss = 0.07544330\n",
      "Iteration 287, loss = 0.07492267\n",
      "Iteration 288, loss = 0.07444102\n",
      "Iteration 289, loss = 0.07392805\n",
      "Iteration 290, loss = 0.07354450\n",
      "Iteration 291, loss = 0.07310823\n",
      "Iteration 292, loss = 0.07268316\n",
      "Iteration 293, loss = 0.07227968\n",
      "Iteration 294, loss = 0.07192736\n",
      "Iteration 295, loss = 0.07152412\n",
      "Iteration 296, loss = 0.07106967\n",
      "Iteration 297, loss = 0.07060355\n",
      "Iteration 298, loss = 0.07009938\n",
      "Iteration 299, loss = 0.06969432\n",
      "Iteration 300, loss = 0.06924818\n",
      "Iteration 301, loss = 0.06883355\n",
      "Iteration 302, loss = 0.06832430\n",
      "Iteration 303, loss = 0.06794421\n",
      "Iteration 304, loss = 0.06750525\n",
      "Iteration 305, loss = 0.06712225\n",
      "Iteration 306, loss = 0.06672525\n",
      "Iteration 307, loss = 0.06643335\n",
      "Iteration 308, loss = 0.06600248\n",
      "Iteration 309, loss = 0.06555588\n",
      "Iteration 310, loss = 0.06512053\n",
      "Iteration 311, loss = 0.06470686\n",
      "Iteration 312, loss = 0.06429887\n",
      "Iteration 313, loss = 0.06391266\n",
      "Iteration 314, loss = 0.06354165\n",
      "Iteration 315, loss = 0.06312259\n",
      "Iteration 316, loss = 0.06267580\n",
      "Iteration 317, loss = 0.06230485\n",
      "Iteration 318, loss = 0.06198423\n",
      "Iteration 319, loss = 0.06163756\n",
      "Iteration 320, loss = 0.06121863\n",
      "Iteration 321, loss = 0.06083638\n",
      "Iteration 322, loss = 0.06047877\n",
      "Iteration 323, loss = 0.06011744\n",
      "Iteration 324, loss = 0.05984705\n",
      "Iteration 325, loss = 0.05955668\n",
      "Iteration 326, loss = 0.05927875\n",
      "Iteration 327, loss = 0.05901578\n",
      "Iteration 328, loss = 0.05873968\n",
      "Iteration 329, loss = 0.05841862\n",
      "Iteration 330, loss = 0.05809653\n",
      "Iteration 331, loss = 0.05776666\n",
      "Iteration 332, loss = 0.05743200\n",
      "Iteration 333, loss = 0.05702344\n",
      "Iteration 334, loss = 0.05671497\n",
      "Iteration 335, loss = 0.05651278\n",
      "Iteration 336, loss = 0.05606498\n",
      "Iteration 337, loss = 0.05575069\n",
      "Iteration 338, loss = 0.05544095\n",
      "Iteration 339, loss = 0.05510074\n",
      "Iteration 340, loss = 0.05482197\n",
      "Iteration 341, loss = 0.05452517\n",
      "Iteration 342, loss = 0.05420300\n",
      "Iteration 343, loss = 0.05386332\n",
      "Iteration 344, loss = 0.05357297\n",
      "Iteration 345, loss = 0.05321910\n",
      "Iteration 346, loss = 0.05291169\n",
      "Iteration 347, loss = 0.05270164\n",
      "Iteration 348, loss = 0.05243501\n",
      "Iteration 349, loss = 0.05217449\n",
      "Iteration 350, loss = 0.05196949\n",
      "Iteration 351, loss = 0.05156840\n",
      "Iteration 352, loss = 0.05121424\n",
      "Iteration 353, loss = 0.05085053\n",
      "Iteration 354, loss = 0.05054984\n",
      "Iteration 355, loss = 0.05029178\n",
      "Iteration 356, loss = 0.05008442\n",
      "Iteration 357, loss = 0.04980179\n",
      "Iteration 358, loss = 0.04952981\n",
      "Iteration 359, loss = 0.04938174\n",
      "Iteration 360, loss = 0.04913246\n",
      "Iteration 361, loss = 0.04880992\n",
      "Iteration 362, loss = 0.04844341\n",
      "Iteration 363, loss = 0.04822145\n",
      "Iteration 364, loss = 0.04798376\n",
      "Iteration 365, loss = 0.04772413\n",
      "Iteration 366, loss = 0.04756678\n",
      "Iteration 367, loss = 0.04732749\n",
      "Iteration 368, loss = 0.04707682\n",
      "Iteration 369, loss = 0.04676299\n",
      "Iteration 370, loss = 0.04638964\n",
      "Iteration 371, loss = 0.04646144\n",
      "Iteration 372, loss = 0.04615649\n",
      "Iteration 373, loss = 0.04597216\n",
      "Iteration 374, loss = 0.04560329\n",
      "Iteration 375, loss = 0.04530340\n",
      "Iteration 376, loss = 0.04496213\n",
      "Iteration 377, loss = 0.04472158\n",
      "Iteration 378, loss = 0.04448876\n",
      "Iteration 379, loss = 0.04424666\n",
      "Iteration 380, loss = 0.04403794\n",
      "Iteration 381, loss = 0.04382135\n",
      "Iteration 382, loss = 0.04366043\n",
      "Iteration 383, loss = 0.04337238\n",
      "Iteration 384, loss = 0.04312879\n",
      "Iteration 385, loss = 0.04292267\n",
      "Iteration 386, loss = 0.04264984\n",
      "Iteration 387, loss = 0.04241206\n",
      "Iteration 388, loss = 0.04219803\n",
      "Iteration 389, loss = 0.04203799\n",
      "Iteration 390, loss = 0.04186435\n",
      "Iteration 391, loss = 0.04166307\n",
      "Iteration 392, loss = 0.04137182\n",
      "Iteration 393, loss = 0.04109983\n",
      "Iteration 394, loss = 0.04085749\n",
      "Iteration 395, loss = 0.04062683\n",
      "Iteration 396, loss = 0.04042902\n",
      "Iteration 397, loss = 0.04024946\n",
      "Iteration 398, loss = 0.04005896\n",
      "Iteration 399, loss = 0.03991223\n",
      "Iteration 400, loss = 0.03971450\n",
      "Iteration 401, loss = 0.03953485\n",
      "Iteration 402, loss = 0.03926450\n",
      "Iteration 403, loss = 0.03905427\n",
      "Iteration 404, loss = 0.03886490\n",
      "Iteration 405, loss = 0.03880114\n",
      "Iteration 406, loss = 0.03863888\n",
      "Iteration 407, loss = 0.03851779\n",
      "Iteration 408, loss = 0.03835076\n",
      "Iteration 409, loss = 0.03808738\n",
      "Iteration 410, loss = 0.03785963\n",
      "Iteration 411, loss = 0.03768482\n",
      "Iteration 412, loss = 0.03755398\n",
      "Iteration 413, loss = 0.03738255\n",
      "Iteration 414, loss = 0.03716306\n",
      "Iteration 415, loss = 0.03700643\n",
      "Iteration 416, loss = 0.03683774\n",
      "Iteration 417, loss = 0.03665195\n",
      "Iteration 418, loss = 0.03647498\n",
      "Iteration 419, loss = 0.03646901\n",
      "Iteration 420, loss = 0.03645149\n",
      "Iteration 421, loss = 0.03641493\n",
      "Iteration 422, loss = 0.03634021\n",
      "Iteration 423, loss = 0.03621667\n",
      "Iteration 424, loss = 0.03605022\n",
      "Iteration 425, loss = 0.03578588\n",
      "Iteration 426, loss = 0.03554358\n",
      "Iteration 427, loss = 0.03532256\n",
      "Iteration 428, loss = 0.03501485\n",
      "Iteration 429, loss = 0.03481721\n",
      "Iteration 430, loss = 0.03459506\n",
      "Iteration 431, loss = 0.03442036\n",
      "Iteration 432, loss = 0.03423003\n",
      "Iteration 433, loss = 0.03403543\n",
      "Iteration 434, loss = 0.03387178\n",
      "Iteration 435, loss = 0.03376930\n",
      "Iteration 436, loss = 0.03351276\n",
      "Iteration 437, loss = 0.03331844\n",
      "Iteration 438, loss = 0.03311246\n",
      "Iteration 439, loss = 0.03302863\n",
      "Iteration 440, loss = 0.03303309\n",
      "Iteration 441, loss = 0.03318732\n",
      "Iteration 442, loss = 0.03291643\n",
      "Iteration 443, loss = 0.03278262\n",
      "Iteration 444, loss = 0.03240339\n",
      "Iteration 445, loss = 0.03220284\n",
      "Iteration 446, loss = 0.03203578\n",
      "Iteration 447, loss = 0.03187462\n",
      "Iteration 448, loss = 0.03174228\n",
      "Iteration 449, loss = 0.03163141\n",
      "Iteration 450, loss = 0.03145431\n",
      "Iteration 451, loss = 0.03128101\n",
      "Iteration 452, loss = 0.03111502\n",
      "Iteration 453, loss = 0.03096101\n",
      "Iteration 454, loss = 0.03081097\n",
      "Iteration 455, loss = 0.03072144\n",
      "Iteration 456, loss = 0.03062173\n",
      "Iteration 457, loss = 0.03050226\n",
      "Iteration 458, loss = 0.03037666\n",
      "Iteration 459, loss = 0.03024032\n",
      "Iteration 460, loss = 0.03010006\n",
      "Iteration 461, loss = 0.02994891\n",
      "Iteration 462, loss = 0.02974956\n",
      "Iteration 463, loss = 0.02962736\n",
      "Iteration 464, loss = 0.02950070\n",
      "Iteration 465, loss = 0.02938621\n",
      "Iteration 466, loss = 0.02928753\n",
      "Iteration 467, loss = 0.02919749\n",
      "Iteration 468, loss = 0.02907426\n",
      "Iteration 469, loss = 0.02894976\n",
      "Iteration 470, loss = 0.02881539\n",
      "Iteration 471, loss = 0.02867465\n",
      "Iteration 472, loss = 0.02864068\n",
      "Iteration 473, loss = 0.02843335\n",
      "Iteration 474, loss = 0.02830019\n",
      "Iteration 475, loss = 0.02817612\n",
      "Iteration 476, loss = 0.02811555\n",
      "Iteration 477, loss = 0.02789066\n",
      "Iteration 478, loss = 0.02774849\n",
      "Iteration 479, loss = 0.02760538\n",
      "Iteration 480, loss = 0.02748855\n",
      "Iteration 481, loss = 0.02740273\n",
      "Iteration 482, loss = 0.02728951\n",
      "Iteration 483, loss = 0.02730125\n",
      "Iteration 484, loss = 0.02727389\n",
      "Iteration 485, loss = 0.02725256\n",
      "Iteration 486, loss = 0.02720870\n",
      "Iteration 487, loss = 0.02714691\n",
      "Iteration 488, loss = 0.02704572\n",
      "Iteration 489, loss = 0.02692992\n",
      "Iteration 490, loss = 0.02679747\n",
      "Iteration 491, loss = 0.02667236\n",
      "Iteration 492, loss = 0.02651613\n",
      "Iteration 493, loss = 0.02643747\n",
      "Iteration 494, loss = 0.02635213\n",
      "Iteration 495, loss = 0.02630833\n",
      "Iteration 496, loss = 0.02621403\n",
      "Iteration 497, loss = 0.02609820\n",
      "Iteration 498, loss = 0.02575498\n",
      "Iteration 499, loss = 0.02582685\n",
      "Iteration 500, loss = 0.02548379\n",
      "Iteration 501, loss = 0.02551131\n",
      "Iteration 502, loss = 0.02530939\n",
      "Iteration 503, loss = 0.02514491\n",
      "Iteration 504, loss = 0.02500787\n",
      "Iteration 505, loss = 0.02495627\n",
      "Iteration 506, loss = 0.02492066\n",
      "Iteration 507, loss = 0.02489444\n",
      "Iteration 508, loss = 0.02493317\n",
      "Iteration 509, loss = 0.02474084\n",
      "Iteration 510, loss = 0.02457210\n",
      "Iteration 511, loss = 0.02445538\n",
      "Iteration 512, loss = 0.02430897\n",
      "Iteration 513, loss = 0.02421757\n",
      "Iteration 514, loss = 0.02412337\n",
      "Iteration 515, loss = 0.02402682\n",
      "Iteration 516, loss = 0.02393021\n",
      "Iteration 517, loss = 0.02385841\n",
      "Iteration 518, loss = 0.02376560\n",
      "Iteration 519, loss = 0.02366942\n",
      "Iteration 520, loss = 0.02357150\n",
      "Iteration 521, loss = 0.02347035\n",
      "Iteration 522, loss = 0.02337508\n",
      "Iteration 523, loss = 0.02330727\n",
      "Iteration 524, loss = 0.02324198\n",
      "Iteration 525, loss = 0.02317480\n",
      "Iteration 526, loss = 0.02308821\n",
      "Iteration 527, loss = 0.02299483\n",
      "Iteration 528, loss = 0.02290392\n",
      "Iteration 529, loss = 0.02283297\n",
      "Iteration 530, loss = 0.02273917\n",
      "Iteration 531, loss = 0.02267655\n",
      "Iteration 532, loss = 0.02258232\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "clf = deepcopy(classifier)\n",
    "clf.fit(Xtrain.fillna(Xtrain.mean()), ytrain)\n",
    "pred = clf.predict(Xtest.fillna(Xtrain.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "scores['mean', 'f1'] = f1_score(ytest, pred, average='macro')\n",
    "scores['mean', 'accuracy'] = accuracy_score(ytest, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_impute(estimator, ds_train, ds_test, iters=1):\n",
    "    missing = ds_train.isna()\n",
    "    df = ds_train.fillna(ds_train.mean())\n",
    "    missing2 = ds_test.isna()\n",
    "    df2 = ds_test.fillna(ds_train.mean())\n",
    "    estimators = {}\n",
    "    for _ in range(iters):\n",
    "        for col in df.columns:\n",
    "            if missing[col].any():\n",
    "                print(\"Imputing Feature\", col)\n",
    "                mask = missing[col]\n",
    "                Xtrain, ytrain = df.drop(col, axis=1)[~mask], df[col][~mask]\n",
    "                est = estimators.setdefault(col, deepcopy(estimator))\n",
    "                est.fit(Xtrain, ytrain)\n",
    "                current = df[col]\n",
    "                df[col] = current.mask(mask, est.predict(df.drop(col, axis=1)))\n",
    "                print(\"Delta:\", mean_absolute_error(df[col], current))\n",
    "                df2[col] = df2[col].mask(missing2[col], est.predict(df2.drop(col, axis=1)))\n",
    "                estimators[col] = est\n",
    "    return df, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing Feature 0\n",
      "Delta: 0.001484889979948109\n",
      "Imputing Feature 3\n",
      "Delta: 0.04408703529346587\n",
      "Imputing Feature 4\n",
      "Delta: 1.544621381010714\n",
      "Imputing Feature 5\n",
      "Delta: 1.3237398381107994\n",
      "Imputing Feature 6\n",
      "Delta: 0.04379634812998547\n",
      "Imputing Feature 7\n",
      "Delta: 0.10539708272242677\n",
      "Imputing Feature 8\n",
      "Delta: 0.10261144402272886\n",
      "Imputing Feature 9\n",
      "Delta: 0.02653876986269139\n",
      "Imputing Feature 10\n",
      "Delta: 0.08806155117076514\n",
      "Imputing Feature 11\n",
      "Delta: 0.04841870628172907\n",
      "Imputing Feature 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta: 0.09067005871709324\n",
      "Imputing Feature 13\n",
      "Delta: 0.07099149796454624\n",
      "Imputing Feature 14\n",
      "Delta: 0.11467329448009538\n",
      "Imputing Feature 15\n",
      "Delta: 1.1327831850079007\n",
      "Imputing Feature 16\n",
      "Delta: 0.15238100159864904\n",
      "Imputing Feature 17\n",
      "Delta: 0.3133616418586664\n",
      "Imputing Feature 18\n",
      "Delta: 0.5185258295702163\n",
      "Imputing Feature 19\n",
      "Delta: 0.3516190089996564\n",
      "Imputing Feature 20\n",
      "Delta: 0.25238925665014267\n",
      "Imputing Feature 21\n",
      "Delta: 0.5012787625470454\n",
      "Imputing Feature 22\n",
      "Delta: 0.00016516334993846028\n",
      "Imputing Feature 0\n",
      "Delta: 0.00042206346365390816\n",
      "Imputing Feature 3\n",
      "Delta: 0.029526837041208894\n",
      "Imputing Feature 4\n",
      "Delta: 0.34213502183796946\n",
      "Imputing Feature 5\n",
      "Delta: 0.971068322003929\n",
      "Imputing Feature 6\n",
      "Delta: 0.03031176398570382\n",
      "Imputing Feature 7\n",
      "Delta: 0.024966403050252252\n",
      "Imputing Feature 8\n",
      "Delta: 0.027270149607636178\n",
      "Imputing Feature 9\n",
      "Delta: 0.006946371930798241\n",
      "Imputing Feature 10\n",
      "Delta: 0.03332590400742705\n",
      "Imputing Feature 11\n",
      "Delta: 0.013459634036646849\n",
      "Imputing Feature 12\n",
      "Delta: 0.02858805539331207\n",
      "Imputing Feature 13\n",
      "Delta: 0.026826100818099833\n",
      "Imputing Feature 14\n",
      "Delta: 0.038269235345780826\n",
      "Imputing Feature 15\n",
      "Delta: 0.338000840413782\n",
      "Imputing Feature 16\n",
      "Delta: 0.04770646730362577\n",
      "Imputing Feature 17\n",
      "Delta: 0.08023426233477343\n",
      "Imputing Feature 18\n",
      "Delta: 0.13626793774019416\n",
      "Imputing Feature 19\n",
      "Delta: 0.16242171111124476\n",
      "Imputing Feature 20\n",
      "Delta: 0.023289350458715462\n",
      "Imputing Feature 21\n",
      "Delta: 0.21654340863574717\n",
      "Imputing Feature 22\n",
      "Delta: 2.3745808244165132e-05\n",
      "Imputing Feature 0\n",
      "Delta: 8.557525153336336e-05\n",
      "Imputing Feature 3\n",
      "Delta: 0.013338120097318\n",
      "Imputing Feature 4\n",
      "Delta: 2.189031218236216\n",
      "Imputing Feature 5\n",
      "Delta: 0.9362637894963621\n",
      "Imputing Feature 6\n",
      "Delta: 0.03343771647959271\n",
      "Imputing Feature 7\n",
      "Delta: 0.03062363466964357\n",
      "Imputing Feature 8\n",
      "Delta: 0.045122146958768705\n",
      "Imputing Feature 9\n",
      "Delta: 0.003048180974917114\n",
      "Imputing Feature 10\n",
      "Delta: 0.02711640489624155\n",
      "Imputing Feature 11\n",
      "Delta: 0.019162453847023656\n",
      "Imputing Feature 12\n",
      "Delta: 0.02849909863695314\n",
      "Imputing Feature 13\n",
      "Delta: 0.01837498359447057\n",
      "Imputing Feature 14\n",
      "Delta: 0.027400548189402563\n",
      "Imputing Feature 15\n",
      "Delta: 0.2391595872725783\n",
      "Imputing Feature 16\n",
      "Delta: 0.12639347105114082\n",
      "Imputing Feature 17\n",
      "Delta: 0.10499297738071883\n",
      "Imputing Feature 18\n",
      "Delta: 0.05771311821115391\n",
      "Imputing Feature 19\n",
      "Delta: 0.17087994124942602\n",
      "Imputing Feature 20\n",
      "Delta: 0.14818936184645468\n",
      "Imputing Feature 21\n",
      "Delta: 0.14158209343854242\n",
      "Imputing Feature 22\n",
      "Delta: 0.0011828333732347993\n",
      "Imputing Feature 0\n",
      "Delta: 0.00029989737274692304\n",
      "Imputing Feature 3\n",
      "Delta: 0.08481492293771703\n",
      "Imputing Feature 4\n",
      "Delta: 0.7686803617549025\n",
      "Imputing Feature 5\n",
      "Delta: 1.82823610596325\n",
      "Imputing Feature 6\n",
      "Delta: 0.07171897184912691\n",
      "Imputing Feature 7\n",
      "Delta: 0.08041494024508497\n",
      "Imputing Feature 8\n",
      "Delta: 0.0525493370338602\n",
      "Imputing Feature 9\n",
      "Delta: 0.007434823879073103\n",
      "Imputing Feature 10\n",
      "Delta: 0.034909316138311164\n",
      "Imputing Feature 11\n",
      "Delta: 0.019208974678368275\n",
      "Imputing Feature 12\n",
      "Delta: 0.02703686712114027\n",
      "Imputing Feature 13\n",
      "Delta: 0.08649217561704373\n",
      "Imputing Feature 14\n",
      "Delta: 0.027582352319183587\n",
      "Imputing Feature 15\n",
      "Delta: 0.15088494086503382\n",
      "Imputing Feature 16\n",
      "Delta: 0.11924843738779693\n",
      "Imputing Feature 17\n",
      "Delta: 0.05522925540224159\n",
      "Imputing Feature 18\n",
      "Delta: 0.23939462070664566\n",
      "Imputing Feature 19\n",
      "Delta: 1.2606335209982855\n",
      "Imputing Feature 20\n",
      "Delta: 0.10965572489460602\n",
      "Imputing Feature 21\n",
      "Delta: 0.10712015676179591\n",
      "Imputing Feature 22\n",
      "Delta: 0.0007739425557676455\n",
      "Imputing Feature 0\n",
      "Delta: 0.0004743253560274663\n",
      "Imputing Feature 3\n",
      "Delta: 0.09964327437836393\n",
      "Imputing Feature 4\n",
      "Delta: 3.2035023443250226\n",
      "Imputing Feature 5\n",
      "Delta: 1.2449860215832962\n",
      "Imputing Feature 6\n",
      "Delta: 0.029521112219989442\n",
      "Imputing Feature 7\n",
      "Delta: 0.045172616899209796\n",
      "Imputing Feature 8\n",
      "Delta: 0.07152067002939824\n",
      "Imputing Feature 9\n",
      "Delta: 0.008253437487651115\n",
      "Imputing Feature 10\n",
      "Delta: 0.0410724841472538\n",
      "Imputing Feature 11\n",
      "Delta: 0.0546163005015187\n",
      "Imputing Feature 12\n",
      "Delta: 0.02819818609485562\n",
      "Imputing Feature 13\n",
      "Delta: 0.07123101835831266\n",
      "Imputing Feature 14\n",
      "Delta: 0.038765680799350284\n",
      "Imputing Feature 15\n",
      "Delta: 0.18615067642995584\n",
      "Imputing Feature 16\n",
      "Delta: 0.0660354447304873\n",
      "Imputing Feature 17\n",
      "Delta: 0.07744276714603068\n",
      "Imputing Feature 18\n",
      "Delta: 0.5013963416339473\n",
      "Imputing Feature 19\n",
      "Delta: 0.406909931864046\n",
      "Imputing Feature 20\n",
      "Delta: 0.15760325395720684\n",
      "Imputing Feature 21\n",
      "Delta: 0.1175417186319607\n",
      "Imputing Feature 22\n",
      "Delta: 0.0006497627667387095\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = iterative_impute(\n",
    "    RidgeCV(),\n",
    "    pd.concat([Xtrain, ytrain], axis=1), \n",
    "    pd.concat([Xtest, ytest], axis=1), \n",
    "    iters=5)\n",
    "Xtrain_imputed = df_train.iloc[:, :-1]\n",
    "Xtest_imputed = df_test.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70194376\n",
      "Iteration 2, loss = 0.67564659\n",
      "Iteration 3, loss = 0.65287017\n",
      "Iteration 4, loss = 0.63321914\n",
      "Iteration 5, loss = 0.61627325\n",
      "Iteration 6, loss = 0.60020313\n",
      "Iteration 7, loss = 0.58634264\n",
      "Iteration 8, loss = 0.57410551\n",
      "Iteration 9, loss = 0.56238799\n",
      "Iteration 10, loss = 0.55179907\n",
      "Iteration 11, loss = 0.54258701\n",
      "Iteration 12, loss = 0.53334667\n",
      "Iteration 13, loss = 0.52534941\n",
      "Iteration 14, loss = 0.51792073\n",
      "Iteration 15, loss = 0.51006766\n",
      "Iteration 16, loss = 0.50338793\n",
      "Iteration 17, loss = 0.49644137\n",
      "Iteration 18, loss = 0.49006985\n",
      "Iteration 19, loss = 0.48406903\n",
      "Iteration 20, loss = 0.47824161\n",
      "Iteration 21, loss = 0.47257019\n",
      "Iteration 22, loss = 0.46743742\n",
      "Iteration 23, loss = 0.46229016\n",
      "Iteration 24, loss = 0.45739168\n",
      "Iteration 25, loss = 0.45303772\n",
      "Iteration 26, loss = 0.44820177\n",
      "Iteration 27, loss = 0.44385361\n",
      "Iteration 28, loss = 0.43948914\n",
      "Iteration 29, loss = 0.43544749\n",
      "Iteration 30, loss = 0.43148465\n",
      "Iteration 31, loss = 0.42755231\n",
      "Iteration 32, loss = 0.42384528\n",
      "Iteration 33, loss = 0.42019899\n",
      "Iteration 34, loss = 0.41735452\n",
      "Iteration 35, loss = 0.41389687\n",
      "Iteration 36, loss = 0.41088828\n",
      "Iteration 37, loss = 0.40783197\n",
      "Iteration 38, loss = 0.40497412\n",
      "Iteration 39, loss = 0.40211427\n",
      "Iteration 40, loss = 0.39895443\n",
      "Iteration 41, loss = 0.39610764\n",
      "Iteration 42, loss = 0.39293748\n",
      "Iteration 43, loss = 0.38989113\n",
      "Iteration 44, loss = 0.38698396\n",
      "Iteration 45, loss = 0.38399111\n",
      "Iteration 46, loss = 0.38139430\n",
      "Iteration 47, loss = 0.37855687\n",
      "Iteration 48, loss = 0.37637077\n",
      "Iteration 49, loss = 0.37415444\n",
      "Iteration 50, loss = 0.37172421\n",
      "Iteration 51, loss = 0.36928261\n",
      "Iteration 52, loss = 0.36702256\n",
      "Iteration 53, loss = 0.36450542\n",
      "Iteration 54, loss = 0.36224993\n",
      "Iteration 55, loss = 0.35989462\n",
      "Iteration 56, loss = 0.35754750\n",
      "Iteration 57, loss = 0.35510407\n",
      "Iteration 58, loss = 0.35286384\n",
      "Iteration 59, loss = 0.35046267\n",
      "Iteration 60, loss = 0.34838738\n",
      "Iteration 61, loss = 0.34626091\n",
      "Iteration 62, loss = 0.34456588\n",
      "Iteration 63, loss = 0.34236834\n",
      "Iteration 64, loss = 0.34032030\n",
      "Iteration 65, loss = 0.33823447\n",
      "Iteration 66, loss = 0.33649685\n",
      "Iteration 67, loss = 0.33436376\n",
      "Iteration 68, loss = 0.33234466\n",
      "Iteration 69, loss = 0.33043123\n",
      "Iteration 70, loss = 0.32859709\n",
      "Iteration 71, loss = 0.32697453\n",
      "Iteration 72, loss = 0.32564052\n",
      "Iteration 73, loss = 0.32424433\n",
      "Iteration 74, loss = 0.32245202\n",
      "Iteration 75, loss = 0.32078285\n",
      "Iteration 76, loss = 0.31881112\n",
      "Iteration 77, loss = 0.31697505\n",
      "Iteration 78, loss = 0.31503716\n",
      "Iteration 79, loss = 0.31308237\n",
      "Iteration 80, loss = 0.31087204\n",
      "Iteration 81, loss = 0.30849188\n",
      "Iteration 82, loss = 0.30627999\n",
      "Iteration 83, loss = 0.30447553\n",
      "Iteration 84, loss = 0.30214876\n",
      "Iteration 85, loss = 0.30023772\n",
      "Iteration 86, loss = 0.29851060\n",
      "Iteration 87, loss = 0.29691128\n",
      "Iteration 88, loss = 0.29568836\n",
      "Iteration 89, loss = 0.29387896\n",
      "Iteration 90, loss = 0.29220046\n",
      "Iteration 91, loss = 0.29062940\n",
      "Iteration 92, loss = 0.28901290\n",
      "Iteration 93, loss = 0.28733509\n",
      "Iteration 94, loss = 0.28550399\n",
      "Iteration 95, loss = 0.28393054\n",
      "Iteration 96, loss = 0.28277966\n",
      "Iteration 97, loss = 0.28125084\n",
      "Iteration 98, loss = 0.27959936\n",
      "Iteration 99, loss = 0.27796721\n",
      "Iteration 100, loss = 0.27640833\n",
      "Iteration 101, loss = 0.27424331\n",
      "Iteration 102, loss = 0.27259083\n",
      "Iteration 103, loss = 0.27094098\n",
      "Iteration 104, loss = 0.26952932\n",
      "Iteration 105, loss = 0.26825470\n",
      "Iteration 106, loss = 0.26686284\n",
      "Iteration 107, loss = 0.26541588\n",
      "Iteration 108, loss = 0.26394050\n",
      "Iteration 109, loss = 0.26232313\n",
      "Iteration 110, loss = 0.26080045\n",
      "Iteration 111, loss = 0.25915638\n",
      "Iteration 112, loss = 0.25770581\n",
      "Iteration 113, loss = 0.25607891\n",
      "Iteration 114, loss = 0.25450605\n",
      "Iteration 115, loss = 0.25303611\n",
      "Iteration 116, loss = 0.25136678\n",
      "Iteration 117, loss = 0.24969114\n",
      "Iteration 118, loss = 0.24840442\n",
      "Iteration 119, loss = 0.24680631\n",
      "Iteration 120, loss = 0.24567071\n",
      "Iteration 121, loss = 0.24425588\n",
      "Iteration 122, loss = 0.24283342\n",
      "Iteration 123, loss = 0.24132492\n",
      "Iteration 124, loss = 0.23998396\n",
      "Iteration 125, loss = 0.23862418\n",
      "Iteration 126, loss = 0.23715268\n",
      "Iteration 127, loss = 0.23581130\n",
      "Iteration 128, loss = 0.23439775\n",
      "Iteration 129, loss = 0.23321370\n",
      "Iteration 130, loss = 0.23183300\n",
      "Iteration 131, loss = 0.23058959\n",
      "Iteration 132, loss = 0.22937375\n",
      "Iteration 133, loss = 0.22807940\n",
      "Iteration 134, loss = 0.22694045\n",
      "Iteration 135, loss = 0.22581293\n",
      "Iteration 136, loss = 0.22442143\n",
      "Iteration 137, loss = 0.22353108\n",
      "Iteration 138, loss = 0.22228539\n",
      "Iteration 139, loss = 0.22116589\n",
      "Iteration 140, loss = 0.21992261\n",
      "Iteration 141, loss = 0.21862161\n",
      "Iteration 142, loss = 0.21732529\n",
      "Iteration 143, loss = 0.21610976\n",
      "Iteration 144, loss = 0.21461264\n",
      "Iteration 145, loss = 0.21341895\n",
      "Iteration 146, loss = 0.21204662\n",
      "Iteration 147, loss = 0.21116605\n",
      "Iteration 148, loss = 0.20994569\n",
      "Iteration 149, loss = 0.20870606\n",
      "Iteration 150, loss = 0.20745708\n",
      "Iteration 151, loss = 0.20578630\n",
      "Iteration 152, loss = 0.20464874\n",
      "Iteration 153, loss = 0.20351628\n",
      "Iteration 154, loss = 0.20232573\n",
      "Iteration 155, loss = 0.20113466\n",
      "Iteration 156, loss = 0.20003018\n",
      "Iteration 157, loss = 0.19897901\n",
      "Iteration 158, loss = 0.19778171\n",
      "Iteration 159, loss = 0.19680191\n",
      "Iteration 160, loss = 0.19552073\n",
      "Iteration 161, loss = 0.19459337\n",
      "Iteration 162, loss = 0.19372528\n",
      "Iteration 163, loss = 0.19254154\n",
      "Iteration 164, loss = 0.19119898\n",
      "Iteration 165, loss = 0.18988236\n",
      "Iteration 166, loss = 0.18856761\n",
      "Iteration 167, loss = 0.18772445\n",
      "Iteration 168, loss = 0.18675458\n",
      "Iteration 169, loss = 0.18624016\n",
      "Iteration 170, loss = 0.18508202\n",
      "Iteration 171, loss = 0.18388313\n",
      "Iteration 172, loss = 0.18287323\n",
      "Iteration 173, loss = 0.18163350\n",
      "Iteration 174, loss = 0.18039789\n",
      "Iteration 175, loss = 0.17963186\n",
      "Iteration 176, loss = 0.17842520\n",
      "Iteration 177, loss = 0.17746344\n",
      "Iteration 178, loss = 0.17655221\n",
      "Iteration 179, loss = 0.17546803\n",
      "Iteration 180, loss = 0.17450798\n",
      "Iteration 181, loss = 0.17354847\n",
      "Iteration 182, loss = 0.17264590\n",
      "Iteration 183, loss = 0.17173958\n",
      "Iteration 184, loss = 0.17086572\n",
      "Iteration 185, loss = 0.16987679\n",
      "Iteration 186, loss = 0.16882921\n",
      "Iteration 187, loss = 0.16812289\n",
      "Iteration 188, loss = 0.16729279\n",
      "Iteration 189, loss = 0.16631924\n",
      "Iteration 190, loss = 0.16531431\n",
      "Iteration 191, loss = 0.16414312\n",
      "Iteration 192, loss = 0.16330543\n",
      "Iteration 193, loss = 0.16246196\n",
      "Iteration 194, loss = 0.16203827\n",
      "Iteration 195, loss = 0.16140553\n",
      "Iteration 196, loss = 0.16058727\n",
      "Iteration 197, loss = 0.15918556\n",
      "Iteration 198, loss = 0.15743916\n",
      "Iteration 199, loss = 0.15620207\n",
      "Iteration 200, loss = 0.15544817\n",
      "Iteration 201, loss = 0.15476581\n",
      "Iteration 202, loss = 0.15400017\n",
      "Iteration 203, loss = 0.15330797\n",
      "Iteration 204, loss = 0.15256426\n",
      "Iteration 205, loss = 0.15159415\n",
      "Iteration 206, loss = 0.15064059\n",
      "Iteration 207, loss = 0.14954840\n",
      "Iteration 208, loss = 0.14850494\n",
      "Iteration 209, loss = 0.14766599\n",
      "Iteration 210, loss = 0.14680420\n",
      "Iteration 211, loss = 0.14573696\n",
      "Iteration 212, loss = 0.14494587\n",
      "Iteration 213, loss = 0.14411010\n",
      "Iteration 214, loss = 0.14357415\n",
      "Iteration 215, loss = 0.14259052\n",
      "Iteration 216, loss = 0.14195223\n",
      "Iteration 217, loss = 0.14127476\n",
      "Iteration 218, loss = 0.14039973\n",
      "Iteration 219, loss = 0.13949963\n",
      "Iteration 220, loss = 0.13872574\n",
      "Iteration 221, loss = 0.13796687\n",
      "Iteration 222, loss = 0.13742589\n",
      "Iteration 223, loss = 0.13677015\n",
      "Iteration 224, loss = 0.13631626\n",
      "Iteration 225, loss = 0.13579165\n",
      "Iteration 226, loss = 0.13520327\n",
      "Iteration 227, loss = 0.13429095\n",
      "Iteration 228, loss = 0.13347260\n",
      "Iteration 229, loss = 0.13270106\n",
      "Iteration 230, loss = 0.13220886\n",
      "Iteration 231, loss = 0.13159059\n",
      "Iteration 232, loss = 0.13084500\n",
      "Iteration 233, loss = 0.12964489\n",
      "Iteration 234, loss = 0.12862000\n",
      "Iteration 235, loss = 0.12770317\n",
      "Iteration 236, loss = 0.12709854\n",
      "Iteration 237, loss = 0.12645776\n",
      "Iteration 238, loss = 0.12636891\n",
      "Iteration 239, loss = 0.12583287\n",
      "Iteration 240, loss = 0.12507889\n",
      "Iteration 241, loss = 0.12392007\n",
      "Iteration 242, loss = 0.12297008\n",
      "Iteration 243, loss = 0.12194898\n",
      "Iteration 244, loss = 0.12144887\n",
      "Iteration 245, loss = 0.12048626\n",
      "Iteration 246, loss = 0.11975318\n",
      "Iteration 247, loss = 0.11920923\n",
      "Iteration 248, loss = 0.11826874\n",
      "Iteration 249, loss = 0.11785859\n",
      "Iteration 250, loss = 0.11729600\n",
      "Iteration 251, loss = 0.11655707\n",
      "Iteration 252, loss = 0.11578226\n",
      "Iteration 253, loss = 0.11536309\n",
      "Iteration 254, loss = 0.11492703\n",
      "Iteration 255, loss = 0.11454516\n",
      "Iteration 256, loss = 0.11410306\n",
      "Iteration 257, loss = 0.11328110\n",
      "Iteration 258, loss = 0.11228995\n",
      "Iteration 259, loss = 0.11140857\n",
      "Iteration 260, loss = 0.11079937\n",
      "Iteration 261, loss = 0.11035681\n",
      "Iteration 262, loss = 0.10984357\n",
      "Iteration 263, loss = 0.10918024\n",
      "Iteration 264, loss = 0.10841387\n",
      "Iteration 265, loss = 0.10768560\n",
      "Iteration 266, loss = 0.10684091\n",
      "Iteration 267, loss = 0.10627732\n",
      "Iteration 268, loss = 0.10566753\n",
      "Iteration 269, loss = 0.10518772\n",
      "Iteration 270, loss = 0.10489371\n",
      "Iteration 271, loss = 0.10463195\n",
      "Iteration 272, loss = 0.10394411\n",
      "Iteration 273, loss = 0.10323670\n",
      "Iteration 274, loss = 0.10247133\n",
      "Iteration 275, loss = 0.10222728\n",
      "Iteration 276, loss = 0.10153987\n",
      "Iteration 277, loss = 0.10094032\n",
      "Iteration 278, loss = 0.10029461\n",
      "Iteration 279, loss = 0.09969760\n",
      "Iteration 280, loss = 0.09926576\n",
      "Iteration 281, loss = 0.09867219\n",
      "Iteration 282, loss = 0.09819816\n",
      "Iteration 283, loss = 0.09761436\n",
      "Iteration 284, loss = 0.09704329\n",
      "Iteration 285, loss = 0.09649556\n",
      "Iteration 286, loss = 0.09595325\n",
      "Iteration 287, loss = 0.09532516\n",
      "Iteration 288, loss = 0.09465246\n",
      "Iteration 289, loss = 0.09408879\n",
      "Iteration 290, loss = 0.09371122\n",
      "Iteration 291, loss = 0.09325592\n",
      "Iteration 292, loss = 0.09270453\n",
      "Iteration 293, loss = 0.09224390\n",
      "Iteration 294, loss = 0.09184915\n",
      "Iteration 295, loss = 0.09145199\n",
      "Iteration 296, loss = 0.09095621\n",
      "Iteration 297, loss = 0.09041594\n",
      "Iteration 298, loss = 0.08983599\n",
      "Iteration 299, loss = 0.08931617\n",
      "Iteration 300, loss = 0.08882518\n",
      "Iteration 301, loss = 0.08825789\n",
      "Iteration 302, loss = 0.08770164\n",
      "Iteration 303, loss = 0.08725276\n",
      "Iteration 304, loss = 0.08672803\n",
      "Iteration 305, loss = 0.08628046\n",
      "Iteration 306, loss = 0.08584302\n",
      "Iteration 307, loss = 0.08558794\n",
      "Iteration 308, loss = 0.08510423\n",
      "Iteration 309, loss = 0.08453124\n",
      "Iteration 310, loss = 0.08398950\n",
      "Iteration 311, loss = 0.08346614\n",
      "Iteration 312, loss = 0.08298654\n",
      "Iteration 313, loss = 0.08247895\n",
      "Iteration 314, loss = 0.08202797\n",
      "Iteration 315, loss = 0.08157454\n",
      "Iteration 316, loss = 0.08106342\n",
      "Iteration 317, loss = 0.08065127\n",
      "Iteration 318, loss = 0.08023262\n",
      "Iteration 319, loss = 0.07986343\n",
      "Iteration 320, loss = 0.07943875\n",
      "Iteration 321, loss = 0.07896466\n",
      "Iteration 322, loss = 0.07859357\n",
      "Iteration 323, loss = 0.07816941\n",
      "Iteration 324, loss = 0.07787271\n",
      "Iteration 325, loss = 0.07753470\n",
      "Iteration 326, loss = 0.07715982\n",
      "Iteration 327, loss = 0.07680803\n",
      "Iteration 328, loss = 0.07641849\n",
      "Iteration 329, loss = 0.07585351\n",
      "Iteration 330, loss = 0.07535238\n",
      "Iteration 331, loss = 0.07494550\n",
      "Iteration 332, loss = 0.07465120\n",
      "Iteration 333, loss = 0.07428936\n",
      "Iteration 334, loss = 0.07408697\n",
      "Iteration 335, loss = 0.07375043\n",
      "Iteration 336, loss = 0.07314296\n",
      "Iteration 337, loss = 0.07258959\n",
      "Iteration 338, loss = 0.07213911\n",
      "Iteration 339, loss = 0.07180895\n",
      "Iteration 340, loss = 0.07160804\n",
      "Iteration 341, loss = 0.07137657\n",
      "Iteration 342, loss = 0.07084992\n",
      "Iteration 343, loss = 0.07029511\n",
      "Iteration 344, loss = 0.06997454\n",
      "Iteration 345, loss = 0.06950675\n",
      "Iteration 346, loss = 0.06912799\n",
      "Iteration 347, loss = 0.06881248\n",
      "Iteration 348, loss = 0.06845160\n",
      "Iteration 349, loss = 0.06814085\n",
      "Iteration 350, loss = 0.06772247\n",
      "Iteration 351, loss = 0.06727809\n",
      "Iteration 352, loss = 0.06693538\n",
      "Iteration 353, loss = 0.06658218\n",
      "Iteration 354, loss = 0.06617949\n",
      "Iteration 355, loss = 0.06596546\n",
      "Iteration 356, loss = 0.06559580\n",
      "Iteration 357, loss = 0.06524814\n",
      "Iteration 358, loss = 0.06489467\n",
      "Iteration 359, loss = 0.06457872\n",
      "Iteration 360, loss = 0.06425365\n",
      "Iteration 361, loss = 0.06385788\n",
      "Iteration 362, loss = 0.06346328\n",
      "Iteration 363, loss = 0.06329704\n",
      "Iteration 364, loss = 0.06305919\n",
      "Iteration 365, loss = 0.06276176\n",
      "Iteration 366, loss = 0.06269715\n",
      "Iteration 367, loss = 0.06230396\n",
      "Iteration 368, loss = 0.06180918\n",
      "Iteration 369, loss = 0.06123708\n",
      "Iteration 370, loss = 0.06092704\n",
      "Iteration 371, loss = 0.06096326\n",
      "Iteration 372, loss = 0.06093243\n",
      "Iteration 373, loss = 0.06077827\n",
      "Iteration 374, loss = 0.06029546\n",
      "Iteration 375, loss = 0.05969173\n",
      "Iteration 376, loss = 0.05914419\n",
      "Iteration 377, loss = 0.05868561\n",
      "Iteration 378, loss = 0.05832706\n",
      "Iteration 379, loss = 0.05799211\n",
      "Iteration 380, loss = 0.05774238\n",
      "Iteration 381, loss = 0.05747949\n",
      "Iteration 382, loss = 0.05714356\n",
      "Iteration 383, loss = 0.05688540\n",
      "Iteration 384, loss = 0.05657205\n",
      "Iteration 385, loss = 0.05624096\n",
      "Iteration 386, loss = 0.05591886\n",
      "Iteration 387, loss = 0.05565110\n",
      "Iteration 388, loss = 0.05534530\n",
      "Iteration 389, loss = 0.05526938\n",
      "Iteration 390, loss = 0.05518131\n",
      "Iteration 391, loss = 0.05496073\n",
      "Iteration 392, loss = 0.05463406\n",
      "Iteration 393, loss = 0.05429314\n",
      "Iteration 394, loss = 0.05391215\n",
      "Iteration 395, loss = 0.05352230\n",
      "Iteration 396, loss = 0.05317890\n",
      "Iteration 397, loss = 0.05286164\n",
      "Iteration 398, loss = 0.05258392\n",
      "Iteration 399, loss = 0.05240165\n",
      "Iteration 400, loss = 0.05212160\n",
      "Iteration 401, loss = 0.05184110\n",
      "Iteration 402, loss = 0.05156672\n",
      "Iteration 403, loss = 0.05140182\n",
      "Iteration 404, loss = 0.05126993\n",
      "Iteration 405, loss = 0.05114402\n",
      "Iteration 406, loss = 0.05087930\n",
      "Iteration 407, loss = 0.05055694\n",
      "Iteration 408, loss = 0.05028204\n",
      "Iteration 409, loss = 0.05003593\n",
      "Iteration 410, loss = 0.04978593\n",
      "Iteration 411, loss = 0.04954381\n",
      "Iteration 412, loss = 0.04934660\n",
      "Iteration 413, loss = 0.04913807\n",
      "Iteration 414, loss = 0.04886868\n",
      "Iteration 415, loss = 0.04866542\n",
      "Iteration 416, loss = 0.04845277\n",
      "Iteration 417, loss = 0.04816072\n",
      "Iteration 418, loss = 0.04789349\n",
      "Iteration 419, loss = 0.04770748\n",
      "Iteration 420, loss = 0.04752270\n",
      "Iteration 421, loss = 0.04737150\n",
      "Iteration 422, loss = 0.04714964\n",
      "Iteration 423, loss = 0.04683107\n",
      "Iteration 424, loss = 0.04656099\n",
      "Iteration 425, loss = 0.04630651\n",
      "Iteration 426, loss = 0.04608942\n",
      "Iteration 427, loss = 0.04597974\n",
      "Iteration 428, loss = 0.04576290\n",
      "Iteration 429, loss = 0.04564247\n",
      "Iteration 430, loss = 0.04541546\n",
      "Iteration 431, loss = 0.04522476\n",
      "Iteration 432, loss = 0.04491455\n",
      "Iteration 433, loss = 0.04461479\n",
      "Iteration 434, loss = 0.04427561\n",
      "Iteration 435, loss = 0.04414433\n",
      "Iteration 436, loss = 0.04395943\n",
      "Iteration 437, loss = 0.04367262\n",
      "Iteration 438, loss = 0.04337002\n",
      "Iteration 439, loss = 0.04320913\n",
      "Iteration 440, loss = 0.04301424\n",
      "Iteration 441, loss = 0.04310233\n",
      "Iteration 442, loss = 0.04291296\n",
      "Iteration 443, loss = 0.04274879\n",
      "Iteration 444, loss = 0.04246997\n",
      "Iteration 445, loss = 0.04222562\n",
      "Iteration 446, loss = 0.04200944\n",
      "Iteration 447, loss = 0.04175004\n",
      "Iteration 448, loss = 0.04149134\n",
      "Iteration 449, loss = 0.04124545\n",
      "Iteration 450, loss = 0.04097025\n",
      "Iteration 451, loss = 0.04082918\n",
      "Iteration 452, loss = 0.04054620\n",
      "Iteration 453, loss = 0.04034906\n",
      "Iteration 454, loss = 0.04015390\n",
      "Iteration 455, loss = 0.04005029\n",
      "Iteration 456, loss = 0.03981724\n",
      "Iteration 457, loss = 0.03959608\n",
      "Iteration 458, loss = 0.03941746\n",
      "Iteration 459, loss = 0.03923113\n",
      "Iteration 460, loss = 0.03904283\n",
      "Iteration 461, loss = 0.03887350\n",
      "Iteration 462, loss = 0.03871094\n",
      "Iteration 463, loss = 0.03861429\n",
      "Iteration 464, loss = 0.03848538\n",
      "Iteration 465, loss = 0.03831957\n",
      "Iteration 466, loss = 0.03808484\n",
      "Iteration 467, loss = 0.03786659\n",
      "Iteration 468, loss = 0.03763364\n",
      "Iteration 469, loss = 0.03751014\n",
      "Iteration 470, loss = 0.03729348\n",
      "Iteration 471, loss = 0.03717364\n",
      "Iteration 472, loss = 0.03710151\n",
      "Iteration 473, loss = 0.03704079\n",
      "Iteration 474, loss = 0.03680793\n",
      "Iteration 475, loss = 0.03668742\n",
      "Iteration 476, loss = 0.03646552\n",
      "Iteration 477, loss = 0.03618792\n",
      "Iteration 478, loss = 0.03597170\n",
      "Iteration 479, loss = 0.03579017\n",
      "Iteration 480, loss = 0.03560466\n",
      "Iteration 481, loss = 0.03544684\n",
      "Iteration 482, loss = 0.03523094\n",
      "Iteration 483, loss = 0.03517800\n",
      "Iteration 484, loss = 0.03500283\n",
      "Iteration 485, loss = 0.03492616\n",
      "Iteration 486, loss = 0.03481778\n",
      "Iteration 487, loss = 0.03472330\n",
      "Iteration 488, loss = 0.03459639\n",
      "Iteration 489, loss = 0.03441447\n",
      "Iteration 490, loss = 0.03422635\n",
      "Iteration 491, loss = 0.03402860\n",
      "Iteration 492, loss = 0.03385899\n",
      "Iteration 493, loss = 0.03368108\n",
      "Iteration 494, loss = 0.03356103\n",
      "Iteration 495, loss = 0.03346008\n",
      "Iteration 496, loss = 0.03332787\n",
      "Iteration 497, loss = 0.03320468\n",
      "Iteration 498, loss = 0.03296756\n",
      "Iteration 499, loss = 0.03279095\n",
      "Iteration 500, loss = 0.03257371\n",
      "Iteration 501, loss = 0.03255660\n",
      "Iteration 502, loss = 0.03234805\n",
      "Iteration 503, loss = 0.03210456\n",
      "Iteration 504, loss = 0.03192213\n",
      "Iteration 505, loss = 0.03190586\n",
      "Iteration 506, loss = 0.03189115\n",
      "Iteration 507, loss = 0.03189466\n",
      "Iteration 508, loss = 0.03187707\n",
      "Iteration 509, loss = 0.03172481\n",
      "Iteration 510, loss = 0.03143181\n",
      "Iteration 511, loss = 0.03124455\n",
      "Iteration 512, loss = 0.03102052\n",
      "Iteration 513, loss = 0.03090571\n",
      "Iteration 514, loss = 0.03074909\n",
      "Iteration 515, loss = 0.03060928\n",
      "Iteration 516, loss = 0.03047692\n",
      "Iteration 517, loss = 0.03034212\n",
      "Iteration 518, loss = 0.03021560\n",
      "Iteration 519, loss = 0.03007304\n",
      "Iteration 520, loss = 0.02991724\n",
      "Iteration 521, loss = 0.02976904\n",
      "Iteration 522, loss = 0.02960077\n",
      "Iteration 523, loss = 0.02948542\n",
      "Iteration 524, loss = 0.02934155\n",
      "Iteration 525, loss = 0.02921751\n",
      "Iteration 526, loss = 0.02909528\n",
      "Iteration 527, loss = 0.02893704\n",
      "Iteration 528, loss = 0.02882653\n",
      "Iteration 529, loss = 0.02877291\n",
      "Iteration 530, loss = 0.02865878\n",
      "Iteration 531, loss = 0.02856136\n",
      "Iteration 532, loss = 0.02841088\n",
      "Iteration 533, loss = 0.02825830\n",
      "Iteration 534, loss = 0.02812657\n",
      "Iteration 535, loss = 0.02801496\n",
      "Iteration 536, loss = 0.02794386\n",
      "Iteration 537, loss = 0.02779119\n",
      "Iteration 538, loss = 0.02769419\n",
      "Iteration 539, loss = 0.02757872\n",
      "Iteration 540, loss = 0.02748811\n",
      "Iteration 541, loss = 0.02734820\n",
      "Iteration 542, loss = 0.02729605\n",
      "Iteration 543, loss = 0.02715911\n",
      "Iteration 544, loss = 0.02706952\n",
      "Iteration 545, loss = 0.02691518\n",
      "Iteration 546, loss = 0.02683612\n",
      "Iteration 547, loss = 0.02672887\n",
      "Iteration 548, loss = 0.02664765\n",
      "Iteration 549, loss = 0.02650066\n",
      "Iteration 550, loss = 0.02637878\n",
      "Iteration 551, loss = 0.02627915\n",
      "Iteration 552, loss = 0.02615991\n",
      "Iteration 553, loss = 0.02607662\n",
      "Iteration 554, loss = 0.02596893\n",
      "Iteration 555, loss = 0.02592372\n",
      "Iteration 556, loss = 0.02582542\n",
      "Iteration 557, loss = 0.02575567\n",
      "Iteration 558, loss = 0.02560870\n",
      "Iteration 559, loss = 0.02548055\n",
      "Iteration 560, loss = 0.02540061\n",
      "Iteration 561, loss = 0.02527915\n",
      "Iteration 562, loss = 0.02520337\n",
      "Iteration 563, loss = 0.02509432\n",
      "Iteration 564, loss = 0.02492819\n",
      "Iteration 565, loss = 0.02482475\n",
      "Iteration 566, loss = 0.02470593\n",
      "Iteration 567, loss = 0.02462532\n",
      "Iteration 568, loss = 0.02452850\n",
      "Iteration 569, loss = 0.02441981\n",
      "Iteration 570, loss = 0.02430358\n",
      "Iteration 571, loss = 0.02421705\n",
      "Iteration 572, loss = 0.02406848\n",
      "Iteration 573, loss = 0.02398759\n",
      "Iteration 574, loss = 0.02389188\n",
      "Iteration 575, loss = 0.02384104\n",
      "Iteration 576, loss = 0.02375070\n",
      "Iteration 577, loss = 0.02365840\n",
      "Iteration 578, loss = 0.02350722\n",
      "Iteration 579, loss = 0.02344017\n",
      "Iteration 580, loss = 0.02332234\n",
      "Iteration 581, loss = 0.02323694\n",
      "Iteration 582, loss = 0.02314884\n",
      "Iteration 583, loss = 0.02307883\n",
      "Iteration 584, loss = 0.02297920\n",
      "Iteration 585, loss = 0.02290282\n",
      "Iteration 586, loss = 0.02280359\n",
      "Iteration 587, loss = 0.02269894\n",
      "Iteration 588, loss = 0.02265916\n",
      "Iteration 589, loss = 0.02265982\n",
      "Iteration 590, loss = 0.02260801\n",
      "Iteration 591, loss = 0.02249911\n",
      "Iteration 592, loss = 0.02236022\n",
      "Iteration 593, loss = 0.02226196\n",
      "Iteration 594, loss = 0.02220072\n",
      "Iteration 595, loss = 0.02207504\n",
      "Iteration 596, loss = 0.02198691\n",
      "Iteration 597, loss = 0.02188357\n",
      "Iteration 598, loss = 0.02180663\n",
      "Iteration 599, loss = 0.02171551\n",
      "Iteration 600, loss = 0.02159017\n",
      "Iteration 601, loss = 0.02148035\n",
      "Iteration 602, loss = 0.02140627\n",
      "Iteration 603, loss = 0.02135636\n",
      "Iteration 604, loss = 0.02129296\n",
      "Iteration 605, loss = 0.02132491\n",
      "Iteration 606, loss = 0.02119405\n",
      "Iteration 607, loss = 0.02107820\n",
      "Iteration 608, loss = 0.02108326\n",
      "Iteration 609, loss = 0.02107597\n",
      "Iteration 610, loss = 0.02102723\n",
      "Iteration 611, loss = 0.02100769\n",
      "Iteration 612, loss = 0.02086947\n",
      "Iteration 613, loss = 0.02067657\n",
      "Iteration 614, loss = 0.02051700\n",
      "Iteration 615, loss = 0.02039908\n",
      "Iteration 616, loss = 0.02029870\n",
      "Iteration 617, loss = 0.02023516\n",
      "Iteration 618, loss = 0.02016791\n",
      "Iteration 619, loss = 0.02012482\n",
      "Iteration 620, loss = 0.02004302\n",
      "Iteration 621, loss = 0.01997723\n",
      "Iteration 622, loss = 0.01986512\n",
      "Iteration 623, loss = 0.01981974\n",
      "Iteration 624, loss = 0.01981787\n",
      "Iteration 625, loss = 0.01973581\n",
      "Iteration 626, loss = 0.01960626\n",
      "Iteration 627, loss = 0.01954055\n",
      "Iteration 628, loss = 0.01940298\n",
      "Iteration 629, loss = 0.01932255\n",
      "Iteration 630, loss = 0.01923385\n",
      "Iteration 631, loss = 0.01918804\n",
      "Iteration 632, loss = 0.01911823\n",
      "Iteration 633, loss = 0.01903865\n",
      "Iteration 634, loss = 0.01892830\n",
      "Iteration 635, loss = 0.01893898\n",
      "Iteration 636, loss = 0.01885070\n",
      "Iteration 637, loss = 0.01875837\n",
      "Iteration 638, loss = 0.01868400\n",
      "Iteration 639, loss = 0.01862451\n",
      "Iteration 640, loss = 0.01849092\n",
      "Iteration 641, loss = 0.01841595\n",
      "Iteration 642, loss = 0.01835336\n",
      "Iteration 643, loss = 0.01825285\n",
      "Iteration 644, loss = 0.01816971\n",
      "Iteration 645, loss = 0.01810247\n",
      "Iteration 646, loss = 0.01802389\n",
      "Iteration 647, loss = 0.01793305\n",
      "Iteration 648, loss = 0.01792076\n",
      "Iteration 649, loss = 0.01783477\n",
      "Iteration 650, loss = 0.01776519\n",
      "Iteration 651, loss = 0.01774382\n",
      "Iteration 652, loss = 0.01768139\n",
      "Iteration 653, loss = 0.01763565\n",
      "Iteration 654, loss = 0.01755760\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "clf = deepcopy(classifier)\n",
    "clf.fit(Xtrain_imputed, ytrain)\n",
    "pred = clf.predict(Xtest_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['ridge', 'f1'] = f1_score(ytest, pred, average='macro')\n",
    "scores['ridge', 'accuracy'] = accuracy_score(ytest, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing Feature 0\n",
      "Delta: 0.0016562064156206415\n",
      "Imputing Feature 3\n",
      "Delta: 0.048926058201057936\n",
      "Imputing Feature 4\n",
      "Delta: 1.6571900452488686\n",
      "Imputing Feature 5\n",
      "Delta: 1.175843537414966\n",
      "Imputing Feature 6\n",
      "Delta: 0.06208934707903778\n",
      "Imputing Feature 7\n",
      "Delta: 0.10666666666666667\n",
      "Imputing Feature 8\n",
      "Delta: 0.11047454844006568\n",
      "Imputing Feature 9\n",
      "Delta: 0.02499538341158059\n",
      "Imputing Feature 10\n",
      "Delta: 0.10625510204081631\n",
      "Imputing Feature 11\n",
      "Delta: 0.043202791461412156\n",
      "Imputing Feature 12\n",
      "Delta: 0.11118197278911565\n",
      "Imputing Feature 13\n",
      "Delta: 0.06391666666666668\n",
      "Imputing Feature 14\n",
      "Delta: 0.14119444444444446\n",
      "Imputing Feature 15\n",
      "Delta: 0.8785416666666666\n",
      "Imputing Feature 16\n",
      "Delta: 0.15135403726708072\n",
      "Imputing Feature 17\n",
      "Delta: 0.2900513698630137\n",
      "Imputing Feature 18\n",
      "Delta: 0.6244600614439323\n",
      "Imputing Feature 19\n",
      "Delta: 1.8654834507042253\n",
      "Imputing Feature 20\n",
      "Delta: 0.22075450450450446\n",
      "Imputing Feature 21\n",
      "Delta: 0.540204430379747\n",
      "Imputing Feature 22\n",
      "Delta: 0.0013556485355648536\n",
      "Imputing Feature 0\n",
      "Delta: 0.0\n",
      "Imputing Feature 3\n",
      "Delta: 0.037324999999999935\n",
      "Imputing Feature 4\n",
      "Delta: 0.4119166666666666\n",
      "Imputing Feature 5\n",
      "Delta: 0.5004166666666668\n",
      "Imputing Feature 6\n",
      "Delta: 0.07325000000000001\n",
      "Imputing Feature 7\n",
      "Delta: 0.05475\n",
      "Imputing Feature 8\n",
      "Delta: 0.07808333333333332\n",
      "Imputing Feature 9\n",
      "Delta: 0.009327777777777777\n",
      "Imputing Feature 10\n",
      "Delta: 0.050166666666666665\n",
      "Imputing Feature 11\n",
      "Delta: 0.033249999999999995\n",
      "Imputing Feature 12\n",
      "Delta: 0.04041666666666667\n",
      "Imputing Feature 13\n",
      "Delta: 0.049499999999999995\n",
      "Imputing Feature 14\n",
      "Delta: 0.07608333333333334\n",
      "Imputing Feature 15\n",
      "Delta: 0.294\n",
      "Imputing Feature 16\n",
      "Delta: 0.11299999999999999\n",
      "Imputing Feature 17\n",
      "Delta: 0.08475000000000002\n",
      "Imputing Feature 18\n",
      "Delta: 0.18549999999999997\n",
      "Imputing Feature 19\n",
      "Delta: 1.0180749999999998\n",
      "Imputing Feature 20\n",
      "Delta: 0.07066666666666668\n",
      "Imputing Feature 21\n",
      "Delta: 0.41385000000000016\n",
      "Imputing Feature 22\n",
      "Delta: 0.0005000000000000004\n",
      "Imputing Feature 0\n",
      "Delta: 8.333333333333341e-05\n",
      "Imputing Feature 3\n",
      "Delta: 0.028916666666666688\n",
      "Imputing Feature 4\n",
      "Delta: 0.23374999999999999\n",
      "Imputing Feature 5\n",
      "Delta: 0.38558333333333333\n",
      "Imputing Feature 6\n",
      "Delta: 0.03983333333333334\n",
      "Imputing Feature 7\n",
      "Delta: 0.026666666666666665\n",
      "Imputing Feature 8\n",
      "Delta: 0.04408333333333333\n",
      "Imputing Feature 9\n",
      "Delta: 0.006583333333333333\n",
      "Imputing Feature 10\n",
      "Delta: 0.03808333333333333\n",
      "Imputing Feature 11\n",
      "Delta: 0.02758333333333333\n",
      "Imputing Feature 12\n",
      "Delta: 0.02766666666666667\n",
      "Imputing Feature 13\n",
      "Delta: 0.044583333333333336\n",
      "Imputing Feature 14\n",
      "Delta: 0.042499999999999996\n",
      "Imputing Feature 15\n",
      "Delta: 0.32736666666666675\n",
      "Imputing Feature 16\n",
      "Delta: 0.10458333333333332\n",
      "Imputing Feature 17\n",
      "Delta: 0.138\n",
      "Imputing Feature 18\n",
      "Delta: 0.1807083333333335\n",
      "Imputing Feature 19\n",
      "Delta: 0.39250833333333346\n",
      "Imputing Feature 20\n",
      "Delta: 0.05741666666666667\n",
      "Imputing Feature 21\n",
      "Delta: 0.2513666666666666\n",
      "Imputing Feature 22\n",
      "Delta: 0.0015833333333333329\n",
      "Imputing Feature 0\n",
      "Delta: 8.333333333333341e-05\n",
      "Imputing Feature 3\n",
      "Delta: 0.027091666666666826\n",
      "Imputing Feature 4\n",
      "Delta: 0.1871666666666666\n",
      "Imputing Feature 5\n",
      "Delta: 0.4530000000000001\n",
      "Imputing Feature 6\n",
      "Delta: 0.037\n",
      "Imputing Feature 7\n",
      "Delta: 0.025083333333333336\n",
      "Imputing Feature 8\n",
      "Delta: 0.04341666666666667\n",
      "Imputing Feature 9\n",
      "Delta: 0.007333333333333333\n",
      "Imputing Feature 10\n",
      "Delta: 0.03808333333333333\n",
      "Imputing Feature 11\n",
      "Delta: 0.020083333333333328\n",
      "Imputing Feature 12\n",
      "Delta: 0.029\n",
      "Imputing Feature 13\n",
      "Delta: 0.055\n",
      "Imputing Feature 14\n",
      "Delta: 0.048499999999999995\n",
      "Imputing Feature 15\n",
      "Delta: 0.15630833333333324\n",
      "Imputing Feature 16\n",
      "Delta: 0.06358333333333333\n",
      "Imputing Feature 17\n",
      "Delta: 0.08424999999999999\n",
      "Imputing Feature 18\n",
      "Delta: 0.1255416666666667\n",
      "Imputing Feature 19\n",
      "Delta: 0.13763333333333339\n",
      "Imputing Feature 20\n",
      "Delta: 0.05783333333333334\n",
      "Imputing Feature 21\n",
      "Delta: 0.20569999999999997\n",
      "Imputing Feature 22\n",
      "Delta: 0.0013333333333333326\n",
      "Imputing Feature 0\n",
      "Delta: 8.333333333333341e-05\n",
      "Imputing Feature 3\n",
      "Delta: 0.028983333333333642\n",
      "Imputing Feature 4\n",
      "Delta: 0.21900000000000006\n",
      "Imputing Feature 5\n",
      "Delta: 0.41691666666666677\n",
      "Imputing Feature 6\n",
      "Delta: 0.03333333333333333\n",
      "Imputing Feature 7\n",
      "Delta: 0.02691666666666666\n",
      "Imputing Feature 8\n",
      "Delta: 0.03133333333333333\n",
      "Imputing Feature 9\n",
      "Delta: 0.004508333333333334\n",
      "Imputing Feature 10\n",
      "Delta: 0.03158333333333333\n",
      "Imputing Feature 11\n",
      "Delta: 0.022583333333333334\n",
      "Imputing Feature 12\n",
      "Delta: 0.026500000000000006\n",
      "Imputing Feature 13\n",
      "Delta: 0.03991666666666667\n",
      "Imputing Feature 14\n",
      "Delta: 0.033\n",
      "Imputing Feature 15\n",
      "Delta: 0.1666583333333333\n",
      "Imputing Feature 16\n",
      "Delta: 0.07374999999999998\n",
      "Imputing Feature 17\n",
      "Delta: 0.07966666666666668\n",
      "Imputing Feature 18\n",
      "Delta: 0.11974999999999998\n",
      "Imputing Feature 19\n",
      "Delta: 0.1573583333333333\n",
      "Imputing Feature 20\n",
      "Delta: 0.046000000000000006\n",
      "Imputing Feature 21\n",
      "Delta: 0.18523333333333333\n",
      "Imputing Feature 22\n",
      "Delta: 0.0010833333333333333\n"
     ]
    }
   ],
   "source": [
    "# MissForest\n",
    "df_train, df_test = iterative_impute(\n",
    "    RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1), \n",
    "    pd.concat([Xtrain, ytrain], axis=1), \n",
    "    pd.concat([Xtest, ytest], axis=1), iters=5)\n",
    "Xtrain_imputed = df_train.iloc[:, :-1]\n",
    "Xtest_imputed = df_test.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69138816\n",
      "Iteration 2, loss = 0.66703567\n",
      "Iteration 3, loss = 0.64672006\n",
      "Iteration 4, loss = 0.62874096\n",
      "Iteration 5, loss = 0.61367800\n",
      "Iteration 6, loss = 0.59941157\n",
      "Iteration 7, loss = 0.58663820\n",
      "Iteration 8, loss = 0.57619926\n",
      "Iteration 9, loss = 0.56559702\n",
      "Iteration 10, loss = 0.55646053\n",
      "Iteration 11, loss = 0.54806710\n",
      "Iteration 12, loss = 0.53967155\n",
      "Iteration 13, loss = 0.53287456\n",
      "Iteration 14, loss = 0.52623311\n",
      "Iteration 15, loss = 0.51927722\n",
      "Iteration 16, loss = 0.51351184\n",
      "Iteration 17, loss = 0.50719777\n",
      "Iteration 18, loss = 0.50186853\n",
      "Iteration 19, loss = 0.49674751\n",
      "Iteration 20, loss = 0.49180639\n",
      "Iteration 21, loss = 0.48707798\n",
      "Iteration 22, loss = 0.48285682\n",
      "Iteration 23, loss = 0.47875888\n",
      "Iteration 24, loss = 0.47459202\n",
      "Iteration 25, loss = 0.47110403\n",
      "Iteration 26, loss = 0.46702290\n",
      "Iteration 27, loss = 0.46339817\n",
      "Iteration 28, loss = 0.45951025\n",
      "Iteration 29, loss = 0.45594722\n",
      "Iteration 30, loss = 0.45240026\n",
      "Iteration 31, loss = 0.44883697\n",
      "Iteration 32, loss = 0.44535679\n",
      "Iteration 33, loss = 0.44200799\n",
      "Iteration 34, loss = 0.43929356\n",
      "Iteration 35, loss = 0.43595809\n",
      "Iteration 36, loss = 0.43293434\n",
      "Iteration 37, loss = 0.42992843\n",
      "Iteration 38, loss = 0.42739822\n",
      "Iteration 39, loss = 0.42451058\n",
      "Iteration 40, loss = 0.42172950\n",
      "Iteration 41, loss = 0.41909246\n",
      "Iteration 42, loss = 0.41608898\n",
      "Iteration 43, loss = 0.41318907\n",
      "Iteration 44, loss = 0.41048452\n",
      "Iteration 45, loss = 0.40755691\n",
      "Iteration 46, loss = 0.40492589\n",
      "Iteration 47, loss = 0.40231477\n",
      "Iteration 48, loss = 0.40006208\n",
      "Iteration 49, loss = 0.39777002\n",
      "Iteration 50, loss = 0.39528845\n",
      "Iteration 51, loss = 0.39286455\n",
      "Iteration 52, loss = 0.39036793\n",
      "Iteration 53, loss = 0.38769924\n",
      "Iteration 54, loss = 0.38520052\n",
      "Iteration 55, loss = 0.38262687\n",
      "Iteration 56, loss = 0.38011893\n",
      "Iteration 57, loss = 0.37755504\n",
      "Iteration 58, loss = 0.37504855\n",
      "Iteration 59, loss = 0.37241327\n",
      "Iteration 60, loss = 0.37003830\n",
      "Iteration 61, loss = 0.36774136\n",
      "Iteration 62, loss = 0.36571961\n",
      "Iteration 63, loss = 0.36334307\n",
      "Iteration 64, loss = 0.36116294\n",
      "Iteration 65, loss = 0.35896869\n",
      "Iteration 66, loss = 0.35705599\n",
      "Iteration 67, loss = 0.35472931\n",
      "Iteration 68, loss = 0.35248461\n",
      "Iteration 69, loss = 0.35025861\n",
      "Iteration 70, loss = 0.34830438\n",
      "Iteration 71, loss = 0.34644218\n",
      "Iteration 72, loss = 0.34501520\n",
      "Iteration 73, loss = 0.34346654\n",
      "Iteration 74, loss = 0.34158040\n",
      "Iteration 75, loss = 0.33973942\n",
      "Iteration 76, loss = 0.33754706\n",
      "Iteration 77, loss = 0.33543583\n",
      "Iteration 78, loss = 0.33307184\n",
      "Iteration 79, loss = 0.33076645\n",
      "Iteration 80, loss = 0.32826387\n",
      "Iteration 81, loss = 0.32572742\n",
      "Iteration 82, loss = 0.32348960\n",
      "Iteration 83, loss = 0.32165783\n",
      "Iteration 84, loss = 0.31945979\n",
      "Iteration 85, loss = 0.31738241\n",
      "Iteration 86, loss = 0.31548059\n",
      "Iteration 87, loss = 0.31352432\n",
      "Iteration 88, loss = 0.31194705\n",
      "Iteration 89, loss = 0.30999320\n",
      "Iteration 90, loss = 0.30811626\n",
      "Iteration 91, loss = 0.30649743\n",
      "Iteration 92, loss = 0.30465181\n",
      "Iteration 93, loss = 0.30274501\n",
      "Iteration 94, loss = 0.30080523\n",
      "Iteration 95, loss = 0.29906381\n",
      "Iteration 96, loss = 0.29722734\n",
      "Iteration 97, loss = 0.29568515\n",
      "Iteration 98, loss = 0.29385117\n",
      "Iteration 99, loss = 0.29205885\n",
      "Iteration 100, loss = 0.29035103\n",
      "Iteration 101, loss = 0.28804811\n",
      "Iteration 102, loss = 0.28640522\n",
      "Iteration 103, loss = 0.28476800\n",
      "Iteration 104, loss = 0.28325528\n",
      "Iteration 105, loss = 0.28183331\n",
      "Iteration 106, loss = 0.28024341\n",
      "Iteration 107, loss = 0.27845936\n",
      "Iteration 108, loss = 0.27662823\n",
      "Iteration 109, loss = 0.27468257\n",
      "Iteration 110, loss = 0.27286877\n",
      "Iteration 111, loss = 0.27109097\n",
      "Iteration 112, loss = 0.26957387\n",
      "Iteration 113, loss = 0.26800392\n",
      "Iteration 114, loss = 0.26691109\n",
      "Iteration 115, loss = 0.26509991\n",
      "Iteration 116, loss = 0.26322694\n",
      "Iteration 117, loss = 0.26136256\n",
      "Iteration 118, loss = 0.25975169\n",
      "Iteration 119, loss = 0.25832111\n",
      "Iteration 120, loss = 0.25740766\n",
      "Iteration 121, loss = 0.25601661\n",
      "Iteration 122, loss = 0.25472180\n",
      "Iteration 123, loss = 0.25340882\n",
      "Iteration 124, loss = 0.25220496\n",
      "Iteration 125, loss = 0.25063344\n",
      "Iteration 126, loss = 0.24899703\n",
      "Iteration 127, loss = 0.24716413\n",
      "Iteration 128, loss = 0.24538063\n",
      "Iteration 129, loss = 0.24359788\n",
      "Iteration 130, loss = 0.24210890\n",
      "Iteration 131, loss = 0.24074054\n",
      "Iteration 132, loss = 0.23941030\n",
      "Iteration 133, loss = 0.23801295\n",
      "Iteration 134, loss = 0.23675945\n",
      "Iteration 135, loss = 0.23530833\n",
      "Iteration 136, loss = 0.23363406\n",
      "Iteration 137, loss = 0.23261932\n",
      "Iteration 138, loss = 0.23113331\n",
      "Iteration 139, loss = 0.22994536\n",
      "Iteration 140, loss = 0.22869852\n",
      "Iteration 141, loss = 0.22743229\n",
      "Iteration 142, loss = 0.22622552\n",
      "Iteration 143, loss = 0.22499935\n",
      "Iteration 144, loss = 0.22341055\n",
      "Iteration 145, loss = 0.22199949\n",
      "Iteration 146, loss = 0.22048306\n",
      "Iteration 147, loss = 0.21898232\n",
      "Iteration 148, loss = 0.21765467\n",
      "Iteration 149, loss = 0.21628327\n",
      "Iteration 150, loss = 0.21502899\n",
      "Iteration 151, loss = 0.21365551\n",
      "Iteration 152, loss = 0.21238242\n",
      "Iteration 153, loss = 0.21106000\n",
      "Iteration 154, loss = 0.20985255\n",
      "Iteration 155, loss = 0.20868222\n",
      "Iteration 156, loss = 0.20753162\n",
      "Iteration 157, loss = 0.20665517\n",
      "Iteration 158, loss = 0.20556890\n",
      "Iteration 159, loss = 0.20442988\n",
      "Iteration 160, loss = 0.20311552\n",
      "Iteration 161, loss = 0.20188666\n",
      "Iteration 162, loss = 0.20055126\n",
      "Iteration 163, loss = 0.19919919\n",
      "Iteration 164, loss = 0.19797644\n",
      "Iteration 165, loss = 0.19699140\n",
      "Iteration 166, loss = 0.19566590\n",
      "Iteration 167, loss = 0.19484545\n",
      "Iteration 168, loss = 0.19389887\n",
      "Iteration 169, loss = 0.19299518\n",
      "Iteration 170, loss = 0.19173876\n",
      "Iteration 171, loss = 0.19034884\n",
      "Iteration 172, loss = 0.18913258\n",
      "Iteration 173, loss = 0.18773149\n",
      "Iteration 174, loss = 0.18654334\n",
      "Iteration 175, loss = 0.18582947\n",
      "Iteration 176, loss = 0.18463200\n",
      "Iteration 177, loss = 0.18365854\n",
      "Iteration 178, loss = 0.18276388\n",
      "Iteration 179, loss = 0.18173138\n",
      "Iteration 180, loss = 0.18077919\n",
      "Iteration 181, loss = 0.17978042\n",
      "Iteration 182, loss = 0.17897470\n",
      "Iteration 183, loss = 0.17764069\n",
      "Iteration 184, loss = 0.17658384\n",
      "Iteration 185, loss = 0.17554644\n",
      "Iteration 186, loss = 0.17444815\n",
      "Iteration 187, loss = 0.17370153\n",
      "Iteration 188, loss = 0.17286629\n",
      "Iteration 189, loss = 0.17200428\n",
      "Iteration 190, loss = 0.17115161\n",
      "Iteration 191, loss = 0.16996267\n",
      "Iteration 192, loss = 0.16903927\n",
      "Iteration 193, loss = 0.16810639\n",
      "Iteration 194, loss = 0.16764082\n",
      "Iteration 195, loss = 0.16684441\n",
      "Iteration 196, loss = 0.16585051\n",
      "Iteration 197, loss = 0.16475997\n",
      "Iteration 198, loss = 0.16324651\n",
      "Iteration 199, loss = 0.16204214\n",
      "Iteration 200, loss = 0.16107456\n",
      "Iteration 201, loss = 0.16038334\n",
      "Iteration 202, loss = 0.15940998\n",
      "Iteration 203, loss = 0.15865291\n",
      "Iteration 204, loss = 0.15779036\n",
      "Iteration 205, loss = 0.15669746\n",
      "Iteration 206, loss = 0.15584643\n",
      "Iteration 207, loss = 0.15470744\n",
      "Iteration 208, loss = 0.15378095\n",
      "Iteration 209, loss = 0.15295942\n",
      "Iteration 210, loss = 0.15202550\n",
      "Iteration 211, loss = 0.15101347\n",
      "Iteration 212, loss = 0.15023307\n",
      "Iteration 213, loss = 0.14951872\n",
      "Iteration 214, loss = 0.14895456\n",
      "Iteration 215, loss = 0.14804216\n",
      "Iteration 216, loss = 0.14736129\n",
      "Iteration 217, loss = 0.14640376\n",
      "Iteration 218, loss = 0.14554689\n",
      "Iteration 219, loss = 0.14461662\n",
      "Iteration 220, loss = 0.14384169\n",
      "Iteration 221, loss = 0.14305509\n",
      "Iteration 222, loss = 0.14213478\n",
      "Iteration 223, loss = 0.14140920\n",
      "Iteration 224, loss = 0.14066542\n",
      "Iteration 225, loss = 0.14022157\n",
      "Iteration 226, loss = 0.13985489\n",
      "Iteration 227, loss = 0.13901536\n",
      "Iteration 228, loss = 0.13816073\n",
      "Iteration 229, loss = 0.13748171\n",
      "Iteration 230, loss = 0.13693936\n",
      "Iteration 231, loss = 0.13614911\n",
      "Iteration 232, loss = 0.13521812\n",
      "Iteration 233, loss = 0.13398545\n",
      "Iteration 234, loss = 0.13289898\n",
      "Iteration 235, loss = 0.13207598\n",
      "Iteration 236, loss = 0.13157179\n",
      "Iteration 237, loss = 0.13083363\n",
      "Iteration 238, loss = 0.13044273\n",
      "Iteration 239, loss = 0.12994494\n",
      "Iteration 240, loss = 0.12931610\n",
      "Iteration 241, loss = 0.12833751\n",
      "Iteration 242, loss = 0.12728407\n",
      "Iteration 243, loss = 0.12627913\n",
      "Iteration 244, loss = 0.12565885\n",
      "Iteration 245, loss = 0.12469663\n",
      "Iteration 246, loss = 0.12397182\n",
      "Iteration 247, loss = 0.12345236\n",
      "Iteration 248, loss = 0.12244683\n",
      "Iteration 249, loss = 0.12176330\n",
      "Iteration 250, loss = 0.12099307\n",
      "Iteration 251, loss = 0.12054842\n",
      "Iteration 252, loss = 0.12008305\n",
      "Iteration 253, loss = 0.11970821\n",
      "Iteration 254, loss = 0.11937773\n",
      "Iteration 255, loss = 0.11900253\n",
      "Iteration 256, loss = 0.11831432\n",
      "Iteration 257, loss = 0.11747929\n",
      "Iteration 258, loss = 0.11626010\n",
      "Iteration 259, loss = 0.11516480\n",
      "Iteration 260, loss = 0.11454853\n",
      "Iteration 261, loss = 0.11420709\n",
      "Iteration 262, loss = 0.11402317\n",
      "Iteration 263, loss = 0.11361991\n",
      "Iteration 264, loss = 0.11301714\n",
      "Iteration 265, loss = 0.11221822\n",
      "Iteration 266, loss = 0.11132117\n",
      "Iteration 267, loss = 0.11046341\n",
      "Iteration 268, loss = 0.10963474\n",
      "Iteration 269, loss = 0.10881713\n",
      "Iteration 270, loss = 0.10834533\n",
      "Iteration 271, loss = 0.10802719\n",
      "Iteration 272, loss = 0.10769216\n",
      "Iteration 273, loss = 0.10719689\n",
      "Iteration 274, loss = 0.10631973\n",
      "Iteration 275, loss = 0.10576844\n",
      "Iteration 276, loss = 0.10498634\n",
      "Iteration 277, loss = 0.10454000\n",
      "Iteration 278, loss = 0.10395899\n",
      "Iteration 279, loss = 0.10339430\n",
      "Iteration 280, loss = 0.10276506\n",
      "Iteration 281, loss = 0.10237647\n",
      "Iteration 282, loss = 0.10180363\n",
      "Iteration 283, loss = 0.10115852\n",
      "Iteration 284, loss = 0.10063886\n",
      "Iteration 285, loss = 0.09993539\n",
      "Iteration 286, loss = 0.09940245\n",
      "Iteration 287, loss = 0.09878602\n",
      "Iteration 288, loss = 0.09822843\n",
      "Iteration 289, loss = 0.09765461\n",
      "Iteration 290, loss = 0.09715957\n",
      "Iteration 291, loss = 0.09656047\n",
      "Iteration 292, loss = 0.09604570\n",
      "Iteration 293, loss = 0.09553881\n",
      "Iteration 294, loss = 0.09509410\n",
      "Iteration 295, loss = 0.09466300\n",
      "Iteration 296, loss = 0.09423019\n",
      "Iteration 297, loss = 0.09375948\n",
      "Iteration 298, loss = 0.09337557\n",
      "Iteration 299, loss = 0.09298133\n",
      "Iteration 300, loss = 0.09239909\n",
      "Iteration 301, loss = 0.09178049\n",
      "Iteration 302, loss = 0.09132333\n",
      "Iteration 303, loss = 0.09087253\n",
      "Iteration 304, loss = 0.09041146\n",
      "Iteration 305, loss = 0.08995299\n",
      "Iteration 306, loss = 0.08954190\n",
      "Iteration 307, loss = 0.08916394\n",
      "Iteration 308, loss = 0.08875899\n",
      "Iteration 309, loss = 0.08822596\n",
      "Iteration 310, loss = 0.08768244\n",
      "Iteration 311, loss = 0.08710955\n",
      "Iteration 312, loss = 0.08663207\n",
      "Iteration 313, loss = 0.08607996\n",
      "Iteration 314, loss = 0.08559621\n",
      "Iteration 315, loss = 0.08518660\n",
      "Iteration 316, loss = 0.08473322\n",
      "Iteration 317, loss = 0.08426287\n",
      "Iteration 318, loss = 0.08382979\n",
      "Iteration 319, loss = 0.08338998\n",
      "Iteration 320, loss = 0.08283261\n",
      "Iteration 321, loss = 0.08238218\n",
      "Iteration 322, loss = 0.08198482\n",
      "Iteration 323, loss = 0.08146850\n",
      "Iteration 324, loss = 0.08107097\n",
      "Iteration 325, loss = 0.08061374\n",
      "Iteration 326, loss = 0.08030027\n",
      "Iteration 327, loss = 0.07993425\n",
      "Iteration 328, loss = 0.07952288\n",
      "Iteration 329, loss = 0.07909790\n",
      "Iteration 330, loss = 0.07868079\n",
      "Iteration 331, loss = 0.07837296\n",
      "Iteration 332, loss = 0.07807380\n",
      "Iteration 333, loss = 0.07760366\n",
      "Iteration 334, loss = 0.07720487\n",
      "Iteration 335, loss = 0.07682917\n",
      "Iteration 336, loss = 0.07634554\n",
      "Iteration 337, loss = 0.07595850\n",
      "Iteration 338, loss = 0.07558941\n",
      "Iteration 339, loss = 0.07524884\n",
      "Iteration 340, loss = 0.07485372\n",
      "Iteration 341, loss = 0.07448966\n",
      "Iteration 342, loss = 0.07406757\n",
      "Iteration 343, loss = 0.07363728\n",
      "Iteration 344, loss = 0.07323336\n",
      "Iteration 345, loss = 0.07293929\n",
      "Iteration 346, loss = 0.07257627\n",
      "Iteration 347, loss = 0.07233773\n",
      "Iteration 348, loss = 0.07192418\n",
      "Iteration 349, loss = 0.07149262\n",
      "Iteration 350, loss = 0.07112139\n",
      "Iteration 351, loss = 0.07079588\n",
      "Iteration 352, loss = 0.07035476\n",
      "Iteration 353, loss = 0.06996045\n",
      "Iteration 354, loss = 0.06951757\n",
      "Iteration 355, loss = 0.06924322\n",
      "Iteration 356, loss = 0.06899342\n",
      "Iteration 357, loss = 0.06868992\n",
      "Iteration 358, loss = 0.06847642\n",
      "Iteration 359, loss = 0.06815675\n",
      "Iteration 360, loss = 0.06785303\n",
      "Iteration 361, loss = 0.06738082\n",
      "Iteration 362, loss = 0.06696605\n",
      "Iteration 363, loss = 0.06692805\n",
      "Iteration 364, loss = 0.06665303\n",
      "Iteration 365, loss = 0.06631380\n",
      "Iteration 366, loss = 0.06616902\n",
      "Iteration 367, loss = 0.06587637\n",
      "Iteration 368, loss = 0.06540474\n",
      "Iteration 369, loss = 0.06475809\n",
      "Iteration 370, loss = 0.06434064\n",
      "Iteration 371, loss = 0.06445375\n",
      "Iteration 372, loss = 0.06440478\n",
      "Iteration 373, loss = 0.06414404\n",
      "Iteration 374, loss = 0.06350785\n",
      "Iteration 375, loss = 0.06297191\n",
      "Iteration 376, loss = 0.06234285\n",
      "Iteration 377, loss = 0.06201839\n",
      "Iteration 378, loss = 0.06180772\n",
      "Iteration 379, loss = 0.06150055\n",
      "Iteration 380, loss = 0.06115502\n",
      "Iteration 381, loss = 0.06081999\n",
      "Iteration 382, loss = 0.06046550\n",
      "Iteration 383, loss = 0.06021334\n",
      "Iteration 384, loss = 0.05988466\n",
      "Iteration 385, loss = 0.05960736\n",
      "Iteration 386, loss = 0.05929283\n",
      "Iteration 387, loss = 0.05898432\n",
      "Iteration 388, loss = 0.05865565\n",
      "Iteration 389, loss = 0.05845299\n",
      "Iteration 390, loss = 0.05821231\n",
      "Iteration 391, loss = 0.05798064\n",
      "Iteration 392, loss = 0.05761638\n",
      "Iteration 393, loss = 0.05730622\n",
      "Iteration 394, loss = 0.05698324\n",
      "Iteration 395, loss = 0.05662468\n",
      "Iteration 396, loss = 0.05637650\n",
      "Iteration 397, loss = 0.05608215\n",
      "Iteration 398, loss = 0.05582686\n",
      "Iteration 399, loss = 0.05563357\n",
      "Iteration 400, loss = 0.05529529\n",
      "Iteration 401, loss = 0.05497162\n",
      "Iteration 402, loss = 0.05468560\n",
      "Iteration 403, loss = 0.05450352\n",
      "Iteration 404, loss = 0.05441684\n",
      "Iteration 405, loss = 0.05432442\n",
      "Iteration 406, loss = 0.05403535\n",
      "Iteration 407, loss = 0.05361497\n",
      "Iteration 408, loss = 0.05336037\n",
      "Iteration 409, loss = 0.05303458\n",
      "Iteration 410, loss = 0.05279851\n",
      "Iteration 411, loss = 0.05252648\n",
      "Iteration 412, loss = 0.05235135\n",
      "Iteration 413, loss = 0.05218578\n",
      "Iteration 414, loss = 0.05197826\n",
      "Iteration 415, loss = 0.05193365\n",
      "Iteration 416, loss = 0.05172315\n",
      "Iteration 417, loss = 0.05143756\n",
      "Iteration 418, loss = 0.05109274\n",
      "Iteration 419, loss = 0.05086613\n",
      "Iteration 420, loss = 0.05076892\n",
      "Iteration 421, loss = 0.05062168\n",
      "Iteration 422, loss = 0.05041323\n",
      "Iteration 423, loss = 0.05008582\n",
      "Iteration 424, loss = 0.04980758\n",
      "Iteration 425, loss = 0.04947318\n",
      "Iteration 426, loss = 0.04921529\n",
      "Iteration 427, loss = 0.04897102\n",
      "Iteration 428, loss = 0.04866910\n",
      "Iteration 429, loss = 0.04842694\n",
      "Iteration 430, loss = 0.04821401\n",
      "Iteration 431, loss = 0.04794448\n",
      "Iteration 432, loss = 0.04771276\n",
      "Iteration 433, loss = 0.04747341\n",
      "Iteration 434, loss = 0.04725383\n",
      "Iteration 435, loss = 0.04712909\n",
      "Iteration 436, loss = 0.04685876\n",
      "Iteration 437, loss = 0.04658346\n",
      "Iteration 438, loss = 0.04627621\n",
      "Iteration 439, loss = 0.04611995\n",
      "Iteration 440, loss = 0.04597074\n",
      "Iteration 441, loss = 0.04607355\n",
      "Iteration 442, loss = 0.04587115\n",
      "Iteration 443, loss = 0.04571263\n",
      "Iteration 444, loss = 0.04544811\n",
      "Iteration 445, loss = 0.04518050\n",
      "Iteration 446, loss = 0.04491188\n",
      "Iteration 447, loss = 0.04454910\n",
      "Iteration 448, loss = 0.04426720\n",
      "Iteration 449, loss = 0.04410016\n",
      "Iteration 450, loss = 0.04385608\n",
      "Iteration 451, loss = 0.04369580\n",
      "Iteration 452, loss = 0.04341231\n",
      "Iteration 453, loss = 0.04320788\n",
      "Iteration 454, loss = 0.04298909\n",
      "Iteration 455, loss = 0.04282326\n",
      "Iteration 456, loss = 0.04263573\n",
      "Iteration 457, loss = 0.04243037\n",
      "Iteration 458, loss = 0.04218534\n",
      "Iteration 459, loss = 0.04193019\n",
      "Iteration 460, loss = 0.04171544\n",
      "Iteration 461, loss = 0.04150831\n",
      "Iteration 462, loss = 0.04128540\n",
      "Iteration 463, loss = 0.04117871\n",
      "Iteration 464, loss = 0.04106583\n",
      "Iteration 465, loss = 0.04086161\n",
      "Iteration 466, loss = 0.04066957\n",
      "Iteration 467, loss = 0.04050690\n",
      "Iteration 468, loss = 0.04031674\n",
      "Iteration 469, loss = 0.04012864\n",
      "Iteration 470, loss = 0.03988403\n",
      "Iteration 471, loss = 0.03970602\n",
      "Iteration 472, loss = 0.03961484\n",
      "Iteration 473, loss = 0.03941772\n",
      "Iteration 474, loss = 0.03920624\n",
      "Iteration 475, loss = 0.03903336\n",
      "Iteration 476, loss = 0.03889294\n",
      "Iteration 477, loss = 0.03863314\n",
      "Iteration 478, loss = 0.03845332\n",
      "Iteration 479, loss = 0.03825973\n",
      "Iteration 480, loss = 0.03804515\n",
      "Iteration 481, loss = 0.03787851\n",
      "Iteration 482, loss = 0.03768820\n",
      "Iteration 483, loss = 0.03759818\n",
      "Iteration 484, loss = 0.03741026\n",
      "Iteration 485, loss = 0.03733939\n",
      "Iteration 486, loss = 0.03736127\n",
      "Iteration 487, loss = 0.03742036\n",
      "Iteration 488, loss = 0.03738921\n",
      "Iteration 489, loss = 0.03724759\n",
      "Iteration 490, loss = 0.03698307\n",
      "Iteration 491, loss = 0.03673293\n",
      "Iteration 492, loss = 0.03654928\n",
      "Iteration 493, loss = 0.03645276\n",
      "Iteration 494, loss = 0.03634632\n",
      "Iteration 495, loss = 0.03617832\n",
      "Iteration 496, loss = 0.03594847\n",
      "Iteration 497, loss = 0.03572506\n",
      "Iteration 498, loss = 0.03547997\n",
      "Iteration 499, loss = 0.03541492\n",
      "Iteration 500, loss = 0.03510205\n",
      "Iteration 501, loss = 0.03494455\n",
      "Iteration 502, loss = 0.03467867\n",
      "Iteration 503, loss = 0.03446471\n",
      "Iteration 504, loss = 0.03439704\n",
      "Iteration 505, loss = 0.03440183\n",
      "Iteration 506, loss = 0.03436178\n",
      "Iteration 507, loss = 0.03427617\n",
      "Iteration 508, loss = 0.03420284\n",
      "Iteration 509, loss = 0.03392218\n",
      "Iteration 510, loss = 0.03363006\n",
      "Iteration 511, loss = 0.03348108\n",
      "Iteration 512, loss = 0.03328957\n",
      "Iteration 513, loss = 0.03317092\n",
      "Iteration 514, loss = 0.03304733\n",
      "Iteration 515, loss = 0.03293473\n",
      "Iteration 516, loss = 0.03281744\n",
      "Iteration 517, loss = 0.03264089\n",
      "Iteration 518, loss = 0.03251148\n",
      "Iteration 519, loss = 0.03229629\n",
      "Iteration 520, loss = 0.03211419\n",
      "Iteration 521, loss = 0.03195656\n",
      "Iteration 522, loss = 0.03185236\n",
      "Iteration 523, loss = 0.03183128\n",
      "Iteration 524, loss = 0.03171144\n",
      "Iteration 525, loss = 0.03156778\n",
      "Iteration 526, loss = 0.03145891\n",
      "Iteration 527, loss = 0.03127828\n",
      "Iteration 528, loss = 0.03112181\n",
      "Iteration 529, loss = 0.03098346\n",
      "Iteration 530, loss = 0.03084917\n",
      "Iteration 531, loss = 0.03075080\n",
      "Iteration 532, loss = 0.03057551\n",
      "Iteration 533, loss = 0.03044380\n",
      "Iteration 534, loss = 0.03029432\n",
      "Iteration 535, loss = 0.03019264\n",
      "Iteration 536, loss = 0.03007657\n",
      "Iteration 537, loss = 0.02994927\n",
      "Iteration 538, loss = 0.02984479\n",
      "Iteration 539, loss = 0.02980732\n",
      "Iteration 540, loss = 0.02974834\n",
      "Iteration 541, loss = 0.02962908\n",
      "Iteration 542, loss = 0.02954429\n",
      "Iteration 543, loss = 0.02942954\n",
      "Iteration 544, loss = 0.02935956\n",
      "Iteration 545, loss = 0.02919643\n",
      "Iteration 546, loss = 0.02909940\n",
      "Iteration 547, loss = 0.02897653\n",
      "Iteration 548, loss = 0.02887197\n",
      "Iteration 549, loss = 0.02872034\n",
      "Iteration 550, loss = 0.02852378\n",
      "Iteration 551, loss = 0.02835194\n",
      "Iteration 552, loss = 0.02811119\n",
      "Iteration 553, loss = 0.02809315\n",
      "Iteration 554, loss = 0.02806396\n",
      "Iteration 555, loss = 0.02809019\n",
      "Iteration 556, loss = 0.02806386\n",
      "Iteration 557, loss = 0.02798654\n",
      "Iteration 558, loss = 0.02785619\n",
      "Iteration 559, loss = 0.02772272\n",
      "Iteration 560, loss = 0.02759064\n",
      "Iteration 561, loss = 0.02740992\n",
      "Iteration 562, loss = 0.02727679\n",
      "Iteration 563, loss = 0.02722755\n",
      "Iteration 564, loss = 0.02714722\n",
      "Iteration 565, loss = 0.02702392\n",
      "Iteration 566, loss = 0.02690747\n",
      "Iteration 567, loss = 0.02668073\n",
      "Iteration 568, loss = 0.02654000\n",
      "Iteration 569, loss = 0.02643958\n",
      "Iteration 570, loss = 0.02632343\n",
      "Iteration 571, loss = 0.02623108\n",
      "Iteration 572, loss = 0.02607649\n",
      "Iteration 573, loss = 0.02596483\n",
      "Iteration 574, loss = 0.02583375\n",
      "Iteration 575, loss = 0.02571824\n",
      "Iteration 576, loss = 0.02560827\n",
      "Iteration 577, loss = 0.02551835\n",
      "Iteration 578, loss = 0.02540573\n",
      "Iteration 579, loss = 0.02533733\n",
      "Iteration 580, loss = 0.02520465\n",
      "Iteration 581, loss = 0.02510530\n",
      "Iteration 582, loss = 0.02501297\n",
      "Iteration 583, loss = 0.02490946\n",
      "Iteration 584, loss = 0.02478738\n",
      "Iteration 585, loss = 0.02467576\n",
      "Iteration 586, loss = 0.02459991\n",
      "Iteration 587, loss = 0.02452038\n",
      "Iteration 588, loss = 0.02452428\n",
      "Iteration 589, loss = 0.02453417\n",
      "Iteration 590, loss = 0.02444270\n",
      "Iteration 591, loss = 0.02432495\n",
      "Iteration 592, loss = 0.02410462\n",
      "Iteration 593, loss = 0.02401046\n",
      "Iteration 594, loss = 0.02394431\n",
      "Iteration 595, loss = 0.02384529\n",
      "Iteration 596, loss = 0.02380027\n",
      "Iteration 597, loss = 0.02376456\n",
      "Iteration 598, loss = 0.02373864\n",
      "Iteration 599, loss = 0.02360471\n",
      "Iteration 600, loss = 0.02344565\n",
      "Iteration 601, loss = 0.02327607\n",
      "Iteration 602, loss = 0.02319387\n",
      "Iteration 603, loss = 0.02312452\n",
      "Iteration 604, loss = 0.02300912\n",
      "Iteration 605, loss = 0.02311255\n",
      "Iteration 606, loss = 0.02296549\n",
      "Iteration 607, loss = 0.02281105\n",
      "Iteration 608, loss = 0.02273240\n",
      "Iteration 609, loss = 0.02268374\n",
      "Iteration 610, loss = 0.02256096\n",
      "Iteration 611, loss = 0.02254282\n",
      "Iteration 612, loss = 0.02238205\n",
      "Iteration 613, loss = 0.02224875\n",
      "Iteration 614, loss = 0.02217811\n",
      "Iteration 615, loss = 0.02211745\n",
      "Iteration 616, loss = 0.02201373\n",
      "Iteration 617, loss = 0.02194113\n",
      "Iteration 618, loss = 0.02186035\n",
      "Iteration 619, loss = 0.02179309\n",
      "Iteration 620, loss = 0.02171084\n",
      "Iteration 621, loss = 0.02166879\n",
      "Iteration 622, loss = 0.02148764\n",
      "Iteration 623, loss = 0.02141160\n",
      "Iteration 624, loss = 0.02133580\n",
      "Iteration 625, loss = 0.02127671\n",
      "Iteration 626, loss = 0.02123948\n",
      "Iteration 627, loss = 0.02116757\n",
      "Iteration 628, loss = 0.02108930\n",
      "Iteration 629, loss = 0.02100971\n",
      "Iteration 630, loss = 0.02093986\n",
      "Iteration 631, loss = 0.02086835\n",
      "Iteration 632, loss = 0.02080600\n",
      "Iteration 633, loss = 0.02070503\n",
      "Iteration 634, loss = 0.02058407\n",
      "Iteration 635, loss = 0.02055440\n",
      "Iteration 636, loss = 0.02045471\n",
      "Iteration 637, loss = 0.02036749\n",
      "Iteration 638, loss = 0.02027998\n",
      "Iteration 639, loss = 0.02025777\n",
      "Iteration 640, loss = 0.02011683\n",
      "Iteration 641, loss = 0.02001344\n",
      "Iteration 642, loss = 0.01991555\n",
      "Iteration 643, loss = 0.01983617\n",
      "Iteration 644, loss = 0.01976815\n",
      "Iteration 645, loss = 0.01972379\n",
      "Iteration 646, loss = 0.01959956\n",
      "Iteration 647, loss = 0.01950311\n",
      "Iteration 648, loss = 0.01944761\n",
      "Iteration 649, loss = 0.01933310\n",
      "Iteration 650, loss = 0.01927826\n",
      "Iteration 651, loss = 0.01925038\n",
      "Iteration 652, loss = 0.01920328\n",
      "Iteration 653, loss = 0.01917139\n",
      "Iteration 654, loss = 0.01909528\n",
      "Iteration 655, loss = 0.01903073\n",
      "Iteration 656, loss = 0.01897376\n",
      "Iteration 657, loss = 0.01887855\n",
      "Iteration 658, loss = 0.01875671\n",
      "Iteration 659, loss = 0.01866746\n",
      "Iteration 660, loss = 0.01864546\n",
      "Iteration 661, loss = 0.01865902\n",
      "Iteration 662, loss = 0.01864572\n",
      "Iteration 663, loss = 0.01861953\n",
      "Iteration 664, loss = 0.01850819\n",
      "Iteration 665, loss = 0.01842694\n",
      "Iteration 666, loss = 0.01839904\n",
      "Iteration 667, loss = 0.01832669\n",
      "Iteration 668, loss = 0.01824066\n",
      "Iteration 669, loss = 0.01815490\n",
      "Iteration 670, loss = 0.01806452\n",
      "Iteration 671, loss = 0.01797429\n",
      "Iteration 672, loss = 0.01786121\n",
      "Iteration 673, loss = 0.01777894\n",
      "Iteration 674, loss = 0.01772085\n",
      "Iteration 675, loss = 0.01766644\n",
      "Iteration 676, loss = 0.01778193\n",
      "Iteration 677, loss = 0.01768557\n",
      "Iteration 678, loss = 0.01759579\n",
      "Iteration 679, loss = 0.01747220\n",
      "Iteration 680, loss = 0.01736672\n",
      "Iteration 681, loss = 0.01730152\n",
      "Iteration 682, loss = 0.01726633\n",
      "Iteration 683, loss = 0.01727777\n",
      "Iteration 684, loss = 0.01730345\n",
      "Iteration 685, loss = 0.01726185\n",
      "Iteration 686, loss = 0.01718709\n",
      "Iteration 687, loss = 0.01708463\n",
      "Iteration 688, loss = 0.01700384\n",
      "Iteration 689, loss = 0.01691901\n",
      "Iteration 690, loss = 0.01685874\n",
      "Iteration 691, loss = 0.01681078\n",
      "Iteration 692, loss = 0.01676076\n",
      "Iteration 693, loss = 0.01669455\n",
      "Iteration 694, loss = 0.01660781\n",
      "Iteration 695, loss = 0.01654353\n",
      "Iteration 696, loss = 0.01648775\n",
      "Iteration 697, loss = 0.01645305\n",
      "Iteration 698, loss = 0.01640869\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "clf = deepcopy(classifier)\n",
    "clf.fit(Xtrain_imputed, ytrain)\n",
    "pred = clf.predict(Xtest_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['missforest', 'f1'] = f1_score(ytest, pred, average='macro')\n",
    "scores['missforest', 'accuracy'] = accuracy_score(ytest, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing Feature 0\n",
      "Delta: 0.0016562064156206415\n",
      "Imputing Feature 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta: 0.05780952380952347\n",
      "Imputing Feature 4\n",
      "Delta: 1.7402300150829562\n",
      "Imputing Feature 5\n",
      "Delta: 1.3055102040816327\n",
      "Imputing Feature 6\n",
      "Delta: 0.08334192439862541\n",
      "Imputing Feature 7\n",
      "Delta: 0.0975\n",
      "Imputing Feature 8\n",
      "Delta: 0.12364121510673236\n",
      "Imputing Feature 9\n",
      "Delta: 0.020414710485133023\n",
      "Imputing Feature 10\n",
      "Delta: 0.12127551020408166\n",
      "Imputing Feature 11\n",
      "Delta: 0.06296387520525452\n",
      "Imputing Feature 12\n",
      "Delta: 0.09499999999999999\n",
      "Imputing Feature 13\n",
      "Delta: 0.08333333333333333\n",
      "Imputing Feature 14\n",
      "Delta: 0.1188888888888889\n",
      "Imputing Feature 15\n",
      "Delta: 1.0090833333333333\n",
      "Imputing Feature 16\n",
      "Delta: 0.14454968944099378\n",
      "Imputing Feature 17\n",
      "Delta: 0.32350456621004564\n",
      "Imputing Feature 18\n",
      "Delta: 0.4952956989247312\n",
      "Imputing Feature 19\n",
      "Delta: 2.5594256651017213\n",
      "Imputing Feature 20\n",
      "Delta: 0.269557057057057\n",
      "Imputing Feature 21\n",
      "Delta: 0.7171339662447258\n",
      "Imputing Feature 22\n",
      "Delta: 0.0011889818688981869\n",
      "Imputing Feature 0\n",
      "Delta: 0.0\n",
      "Imputing Feature 3\n",
      "Delta: 2.960594732333751e-17\n",
      "Imputing Feature 4\n",
      "Delta: 0.0\n",
      "Imputing Feature 5\n",
      "Delta: 0.0\n",
      "Imputing Feature 6\n",
      "Delta: 0.0\n",
      "Imputing Feature 7\n",
      "Delta: 0.0\n",
      "Imputing Feature 8\n",
      "Delta: 0.0\n",
      "Imputing Feature 9\n",
      "Delta: 0.0\n",
      "Imputing Feature 10\n",
      "Delta: 0.0\n",
      "Imputing Feature 11\n",
      "Delta: 0.0\n",
      "Imputing Feature 12\n",
      "Delta: 0.0\n",
      "Imputing Feature 13\n",
      "Delta: 0.0\n",
      "Imputing Feature 14\n",
      "Delta: 0.0\n",
      "Imputing Feature 15\n",
      "Delta: 0.0\n",
      "Imputing Feature 16\n",
      "Delta: 0.0\n",
      "Imputing Feature 17\n",
      "Delta: 0.0\n",
      "Imputing Feature 18\n",
      "Delta: 0.0\n",
      "Imputing Feature 19\n",
      "Delta: 0.0\n",
      "Imputing Feature 20\n",
      "Delta: 0.0\n",
      "Imputing Feature 21\n",
      "Delta: 0.0\n",
      "Imputing Feature 22\n",
      "Delta: 0.0\n",
      "Imputing Feature 0\n",
      "Delta: 0.0\n",
      "Imputing Feature 3\n",
      "Delta: 0.0\n",
      "Imputing Feature 4\n",
      "Delta: 0.0\n",
      "Imputing Feature 5\n",
      "Delta: 0.0\n",
      "Imputing Feature 6\n",
      "Delta: 0.0\n",
      "Imputing Feature 7\n",
      "Delta: 0.0\n",
      "Imputing Feature 8\n",
      "Delta: 0.0\n",
      "Imputing Feature 9\n",
      "Delta: 0.0\n",
      "Imputing Feature 10\n",
      "Delta: 0.0\n",
      "Imputing Feature 11\n",
      "Delta: 0.0\n",
      "Imputing Feature 12\n",
      "Delta: 0.0\n",
      "Imputing Feature 13\n",
      "Delta: 0.0\n",
      "Imputing Feature 14\n",
      "Delta: 0.0\n",
      "Imputing Feature 15\n",
      "Delta: 0.0\n",
      "Imputing Feature 16\n",
      "Delta: 0.0\n",
      "Imputing Feature 17\n",
      "Delta: 0.0\n",
      "Imputing Feature 18\n",
      "Delta: 0.0\n",
      "Imputing Feature 19\n",
      "Delta: 0.0\n",
      "Imputing Feature 20\n",
      "Delta: 0.0\n",
      "Imputing Feature 21\n",
      "Delta: 0.0\n",
      "Imputing Feature 22\n",
      "Delta: 0.0\n",
      "Imputing Feature 0\n",
      "Delta: 0.0\n",
      "Imputing Feature 3\n",
      "Delta: 0.0\n",
      "Imputing Feature 4\n",
      "Delta: 0.0\n",
      "Imputing Feature 5\n",
      "Delta: 0.0\n",
      "Imputing Feature 6\n",
      "Delta: 0.0\n",
      "Imputing Feature 7\n",
      "Delta: 0.0\n",
      "Imputing Feature 8\n",
      "Delta: 0.0\n",
      "Imputing Feature 9\n",
      "Delta: 0.0\n",
      "Imputing Feature 10\n",
      "Delta: 0.0\n",
      "Imputing Feature 11\n",
      "Delta: 0.0\n",
      "Imputing Feature 12\n",
      "Delta: 0.0\n",
      "Imputing Feature 13\n",
      "Delta: 0.0\n",
      "Imputing Feature 14\n",
      "Delta: 0.0\n",
      "Imputing Feature 15\n",
      "Delta: 0.0\n",
      "Imputing Feature 16\n",
      "Delta: 0.0\n",
      "Imputing Feature 17\n",
      "Delta: 0.0\n",
      "Imputing Feature 18\n",
      "Delta: 0.0\n",
      "Imputing Feature 19\n",
      "Delta: 0.0\n",
      "Imputing Feature 20\n",
      "Delta: 0.0\n",
      "Imputing Feature 21\n",
      "Delta: 0.0\n",
      "Imputing Feature 22\n",
      "Delta: 0.0\n",
      "Imputing Feature 0\n",
      "Delta: 0.0\n",
      "Imputing Feature 3\n",
      "Delta: 0.0\n",
      "Imputing Feature 4\n",
      "Delta: 0.0\n",
      "Imputing Feature 5\n",
      "Delta: 0.0\n",
      "Imputing Feature 6\n",
      "Delta: 0.0\n",
      "Imputing Feature 7\n",
      "Delta: 0.0\n",
      "Imputing Feature 8\n",
      "Delta: 0.0\n",
      "Imputing Feature 9\n",
      "Delta: 0.0\n",
      "Imputing Feature 10\n",
      "Delta: 0.0\n",
      "Imputing Feature 11\n",
      "Delta: 0.0\n",
      "Imputing Feature 12\n",
      "Delta: 0.0\n",
      "Imputing Feature 13\n",
      "Delta: 0.0\n",
      "Imputing Feature 14\n",
      "Delta: 0.0\n",
      "Imputing Feature 15\n",
      "Delta: 0.0\n",
      "Imputing Feature 16\n",
      "Delta: 0.0\n",
      "Imputing Feature 17\n",
      "Delta: 0.0\n",
      "Imputing Feature 18\n",
      "Delta: 0.0\n",
      "Imputing Feature 19\n",
      "Delta: 0.0\n",
      "Imputing Feature 20\n",
      "Delta: 0.0\n",
      "Imputing Feature 21\n",
      "Delta: 0.0\n",
      "Imputing Feature 22\n",
      "Delta: 0.0\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = iterative_impute(\n",
    "    KNeighborsRegressor(n_neighbors=5, n_jobs=-1),\n",
    "    pd.concat([Xtrain, ytrain], axis=1), \n",
    "    pd.concat([Xtest, ytest], axis=1), iters=5)\n",
    "Xtrain_imputed = df_train.iloc[:, :-1]\n",
    "Xtest_imputed = df_test.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69138816\n",
      "Iteration 2, loss = 0.66703567\n",
      "Iteration 3, loss = 0.64672006\n",
      "Iteration 4, loss = 0.62874096\n",
      "Iteration 5, loss = 0.61367800\n",
      "Iteration 6, loss = 0.59941157\n",
      "Iteration 7, loss = 0.58663820\n",
      "Iteration 8, loss = 0.57619926\n",
      "Iteration 9, loss = 0.56559702\n",
      "Iteration 10, loss = 0.55646053\n",
      "Iteration 11, loss = 0.54806710\n",
      "Iteration 12, loss = 0.53967155\n",
      "Iteration 13, loss = 0.53287456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14, loss = 0.52623311\n",
      "Iteration 15, loss = 0.51927722\n",
      "Iteration 16, loss = 0.51351184\n",
      "Iteration 17, loss = 0.50719777\n",
      "Iteration 18, loss = 0.50186853\n",
      "Iteration 19, loss = 0.49674751\n",
      "Iteration 20, loss = 0.49180639\n",
      "Iteration 21, loss = 0.48707798\n",
      "Iteration 22, loss = 0.48285682\n",
      "Iteration 23, loss = 0.47875888\n",
      "Iteration 24, loss = 0.47459202\n",
      "Iteration 25, loss = 0.47110403\n",
      "Iteration 26, loss = 0.46702290\n",
      "Iteration 27, loss = 0.46339817\n",
      "Iteration 28, loss = 0.45951025\n",
      "Iteration 29, loss = 0.45594722\n",
      "Iteration 30, loss = 0.45240026\n",
      "Iteration 31, loss = 0.44883697\n",
      "Iteration 32, loss = 0.44535679\n",
      "Iteration 33, loss = 0.44200799\n",
      "Iteration 34, loss = 0.43929356\n",
      "Iteration 35, loss = 0.43595809\n",
      "Iteration 36, loss = 0.43293434\n",
      "Iteration 37, loss = 0.42992843\n",
      "Iteration 38, loss = 0.42739822\n",
      "Iteration 39, loss = 0.42451058\n",
      "Iteration 40, loss = 0.42172950\n",
      "Iteration 41, loss = 0.41909246\n",
      "Iteration 42, loss = 0.41608898\n",
      "Iteration 43, loss = 0.41318907\n",
      "Iteration 44, loss = 0.41048452\n",
      "Iteration 45, loss = 0.40755691\n",
      "Iteration 46, loss = 0.40492589\n",
      "Iteration 47, loss = 0.40231477\n",
      "Iteration 48, loss = 0.40006208\n",
      "Iteration 49, loss = 0.39777002\n",
      "Iteration 50, loss = 0.39528845\n",
      "Iteration 51, loss = 0.39286455\n",
      "Iteration 52, loss = 0.39036793\n",
      "Iteration 53, loss = 0.38769924\n",
      "Iteration 54, loss = 0.38520052\n",
      "Iteration 55, loss = 0.38262687\n",
      "Iteration 56, loss = 0.38011893\n",
      "Iteration 57, loss = 0.37755504\n",
      "Iteration 58, loss = 0.37504855\n",
      "Iteration 59, loss = 0.37241327\n",
      "Iteration 60, loss = 0.37003830\n",
      "Iteration 61, loss = 0.36774136\n",
      "Iteration 62, loss = 0.36571961\n",
      "Iteration 63, loss = 0.36334307\n",
      "Iteration 64, loss = 0.36116294\n",
      "Iteration 65, loss = 0.35896869\n",
      "Iteration 66, loss = 0.35705599\n",
      "Iteration 67, loss = 0.35472931\n",
      "Iteration 68, loss = 0.35248461\n",
      "Iteration 69, loss = 0.35025861\n",
      "Iteration 70, loss = 0.34830438\n",
      "Iteration 71, loss = 0.34644218\n",
      "Iteration 72, loss = 0.34501520\n",
      "Iteration 73, loss = 0.34346654\n",
      "Iteration 74, loss = 0.34158040\n",
      "Iteration 75, loss = 0.33973942\n",
      "Iteration 76, loss = 0.33754706\n",
      "Iteration 77, loss = 0.33543583\n",
      "Iteration 78, loss = 0.33307184\n",
      "Iteration 79, loss = 0.33076645\n",
      "Iteration 80, loss = 0.32826387\n",
      "Iteration 81, loss = 0.32572742\n",
      "Iteration 82, loss = 0.32348960\n",
      "Iteration 83, loss = 0.32165783\n",
      "Iteration 84, loss = 0.31945979\n",
      "Iteration 85, loss = 0.31738241\n",
      "Iteration 86, loss = 0.31548059\n",
      "Iteration 87, loss = 0.31352432\n",
      "Iteration 88, loss = 0.31194705\n",
      "Iteration 89, loss = 0.30999320\n",
      "Iteration 90, loss = 0.30811626\n",
      "Iteration 91, loss = 0.30649743\n",
      "Iteration 92, loss = 0.30465181\n",
      "Iteration 93, loss = 0.30274501\n",
      "Iteration 94, loss = 0.30080523\n",
      "Iteration 95, loss = 0.29906381\n",
      "Iteration 96, loss = 0.29722734\n",
      "Iteration 97, loss = 0.29568515\n",
      "Iteration 98, loss = 0.29385117\n",
      "Iteration 99, loss = 0.29205885\n",
      "Iteration 100, loss = 0.29035103\n",
      "Iteration 101, loss = 0.28804811\n",
      "Iteration 102, loss = 0.28640522\n",
      "Iteration 103, loss = 0.28476800\n",
      "Iteration 104, loss = 0.28325528\n",
      "Iteration 105, loss = 0.28183331\n",
      "Iteration 106, loss = 0.28024341\n",
      "Iteration 107, loss = 0.27845936\n",
      "Iteration 108, loss = 0.27662823\n",
      "Iteration 109, loss = 0.27468257\n",
      "Iteration 110, loss = 0.27286877\n",
      "Iteration 111, loss = 0.27109097\n",
      "Iteration 112, loss = 0.26957387\n",
      "Iteration 113, loss = 0.26800392\n",
      "Iteration 114, loss = 0.26691109\n",
      "Iteration 115, loss = 0.26509991\n",
      "Iteration 116, loss = 0.26322694\n",
      "Iteration 117, loss = 0.26136256\n",
      "Iteration 118, loss = 0.25975169\n",
      "Iteration 119, loss = 0.25832111\n",
      "Iteration 120, loss = 0.25740766\n",
      "Iteration 121, loss = 0.25601661\n",
      "Iteration 122, loss = 0.25472180\n",
      "Iteration 123, loss = 0.25340882\n",
      "Iteration 124, loss = 0.25220496\n",
      "Iteration 125, loss = 0.25063344\n",
      "Iteration 126, loss = 0.24899703\n",
      "Iteration 127, loss = 0.24716413\n",
      "Iteration 128, loss = 0.24538063\n",
      "Iteration 129, loss = 0.24359788\n",
      "Iteration 130, loss = 0.24210890\n",
      "Iteration 131, loss = 0.24074054\n",
      "Iteration 132, loss = 0.23941030\n",
      "Iteration 133, loss = 0.23801295\n",
      "Iteration 134, loss = 0.23675945\n",
      "Iteration 135, loss = 0.23530833\n",
      "Iteration 136, loss = 0.23363406\n",
      "Iteration 137, loss = 0.23261932\n",
      "Iteration 138, loss = 0.23113331\n",
      "Iteration 139, loss = 0.22994536\n",
      "Iteration 140, loss = 0.22869852\n",
      "Iteration 141, loss = 0.22743229\n",
      "Iteration 142, loss = 0.22622552\n",
      "Iteration 143, loss = 0.22499935\n",
      "Iteration 144, loss = 0.22341055\n",
      "Iteration 145, loss = 0.22199949\n",
      "Iteration 146, loss = 0.22048306\n",
      "Iteration 147, loss = 0.21898232\n",
      "Iteration 148, loss = 0.21765467\n",
      "Iteration 149, loss = 0.21628327\n",
      "Iteration 150, loss = 0.21502899\n",
      "Iteration 151, loss = 0.21365551\n",
      "Iteration 152, loss = 0.21238242\n",
      "Iteration 153, loss = 0.21106000\n",
      "Iteration 154, loss = 0.20985255\n",
      "Iteration 155, loss = 0.20868222\n",
      "Iteration 156, loss = 0.20753162\n",
      "Iteration 157, loss = 0.20665517\n",
      "Iteration 158, loss = 0.20556890\n",
      "Iteration 159, loss = 0.20442988\n",
      "Iteration 160, loss = 0.20311552\n",
      "Iteration 161, loss = 0.20188666\n",
      "Iteration 162, loss = 0.20055126\n",
      "Iteration 163, loss = 0.19919919\n",
      "Iteration 164, loss = 0.19797644\n",
      "Iteration 165, loss = 0.19699140\n",
      "Iteration 166, loss = 0.19566590\n",
      "Iteration 167, loss = 0.19484545\n",
      "Iteration 168, loss = 0.19389887\n",
      "Iteration 169, loss = 0.19299518\n",
      "Iteration 170, loss = 0.19173876\n",
      "Iteration 171, loss = 0.19034884\n",
      "Iteration 172, loss = 0.18913258\n",
      "Iteration 173, loss = 0.18773149\n",
      "Iteration 174, loss = 0.18654334\n",
      "Iteration 175, loss = 0.18582947\n",
      "Iteration 176, loss = 0.18463200\n",
      "Iteration 177, loss = 0.18365854\n",
      "Iteration 178, loss = 0.18276388\n",
      "Iteration 179, loss = 0.18173138\n",
      "Iteration 180, loss = 0.18077919\n",
      "Iteration 181, loss = 0.17978042\n",
      "Iteration 182, loss = 0.17897470\n",
      "Iteration 183, loss = 0.17764069\n",
      "Iteration 184, loss = 0.17658384\n",
      "Iteration 185, loss = 0.17554644\n",
      "Iteration 186, loss = 0.17444815\n",
      "Iteration 187, loss = 0.17370153\n",
      "Iteration 188, loss = 0.17286629\n",
      "Iteration 189, loss = 0.17200428\n",
      "Iteration 190, loss = 0.17115161\n",
      "Iteration 191, loss = 0.16996267\n",
      "Iteration 192, loss = 0.16903927\n",
      "Iteration 193, loss = 0.16810639\n",
      "Iteration 194, loss = 0.16764082\n",
      "Iteration 195, loss = 0.16684441\n",
      "Iteration 196, loss = 0.16585051\n",
      "Iteration 197, loss = 0.16475997\n",
      "Iteration 198, loss = 0.16324651\n",
      "Iteration 199, loss = 0.16204214\n",
      "Iteration 200, loss = 0.16107456\n",
      "Iteration 201, loss = 0.16038334\n",
      "Iteration 202, loss = 0.15940998\n",
      "Iteration 203, loss = 0.15865291\n",
      "Iteration 204, loss = 0.15779036\n",
      "Iteration 205, loss = 0.15669746\n",
      "Iteration 206, loss = 0.15584643\n",
      "Iteration 207, loss = 0.15470744\n",
      "Iteration 208, loss = 0.15378095\n",
      "Iteration 209, loss = 0.15295942\n",
      "Iteration 210, loss = 0.15202550\n",
      "Iteration 211, loss = 0.15101347\n",
      "Iteration 212, loss = 0.15023307\n",
      "Iteration 213, loss = 0.14951872\n",
      "Iteration 214, loss = 0.14895456\n",
      "Iteration 215, loss = 0.14804216\n",
      "Iteration 216, loss = 0.14736129\n",
      "Iteration 217, loss = 0.14640376\n",
      "Iteration 218, loss = 0.14554689\n",
      "Iteration 219, loss = 0.14461662\n",
      "Iteration 220, loss = 0.14384169\n",
      "Iteration 221, loss = 0.14305509\n",
      "Iteration 222, loss = 0.14213478\n",
      "Iteration 223, loss = 0.14140920\n",
      "Iteration 224, loss = 0.14066542\n",
      "Iteration 225, loss = 0.14022157\n",
      "Iteration 226, loss = 0.13985489\n",
      "Iteration 227, loss = 0.13901536\n",
      "Iteration 228, loss = 0.13816073\n",
      "Iteration 229, loss = 0.13748171\n",
      "Iteration 230, loss = 0.13693936\n",
      "Iteration 231, loss = 0.13614911\n",
      "Iteration 232, loss = 0.13521812\n",
      "Iteration 233, loss = 0.13398545\n",
      "Iteration 234, loss = 0.13289898\n",
      "Iteration 235, loss = 0.13207598\n",
      "Iteration 236, loss = 0.13157179\n",
      "Iteration 237, loss = 0.13083363\n",
      "Iteration 238, loss = 0.13044273\n",
      "Iteration 239, loss = 0.12994494\n",
      "Iteration 240, loss = 0.12931610\n",
      "Iteration 241, loss = 0.12833751\n",
      "Iteration 242, loss = 0.12728407\n",
      "Iteration 243, loss = 0.12627913\n",
      "Iteration 244, loss = 0.12565885\n",
      "Iteration 245, loss = 0.12469663\n",
      "Iteration 246, loss = 0.12397182\n",
      "Iteration 247, loss = 0.12345236\n",
      "Iteration 248, loss = 0.12244683\n",
      "Iteration 249, loss = 0.12176330\n",
      "Iteration 250, loss = 0.12099307\n",
      "Iteration 251, loss = 0.12054842\n",
      "Iteration 252, loss = 0.12008305\n",
      "Iteration 253, loss = 0.11970821\n",
      "Iteration 254, loss = 0.11937773\n",
      "Iteration 255, loss = 0.11900253\n",
      "Iteration 256, loss = 0.11831432\n",
      "Iteration 257, loss = 0.11747929\n",
      "Iteration 258, loss = 0.11626010\n",
      "Iteration 259, loss = 0.11516480\n",
      "Iteration 260, loss = 0.11454853\n",
      "Iteration 261, loss = 0.11420709\n",
      "Iteration 262, loss = 0.11402317\n",
      "Iteration 263, loss = 0.11361991\n",
      "Iteration 264, loss = 0.11301714\n",
      "Iteration 265, loss = 0.11221822\n",
      "Iteration 266, loss = 0.11132117\n",
      "Iteration 267, loss = 0.11046341\n",
      "Iteration 268, loss = 0.10963474\n",
      "Iteration 269, loss = 0.10881713\n",
      "Iteration 270, loss = 0.10834533\n",
      "Iteration 271, loss = 0.10802719\n",
      "Iteration 272, loss = 0.10769216\n",
      "Iteration 273, loss = 0.10719689\n",
      "Iteration 274, loss = 0.10631973\n",
      "Iteration 275, loss = 0.10576844\n",
      "Iteration 276, loss = 0.10498634\n",
      "Iteration 277, loss = 0.10454000\n",
      "Iteration 278, loss = 0.10395899\n",
      "Iteration 279, loss = 0.10339430\n",
      "Iteration 280, loss = 0.10276506\n",
      "Iteration 281, loss = 0.10237647\n",
      "Iteration 282, loss = 0.10180363\n",
      "Iteration 283, loss = 0.10115852\n",
      "Iteration 284, loss = 0.10063886\n",
      "Iteration 285, loss = 0.09993539\n",
      "Iteration 286, loss = 0.09940245\n",
      "Iteration 287, loss = 0.09878602\n",
      "Iteration 288, loss = 0.09822843\n",
      "Iteration 289, loss = 0.09765461\n",
      "Iteration 290, loss = 0.09715957\n",
      "Iteration 291, loss = 0.09656047\n",
      "Iteration 292, loss = 0.09604570\n",
      "Iteration 293, loss = 0.09553881\n",
      "Iteration 294, loss = 0.09509410\n",
      "Iteration 295, loss = 0.09466300\n",
      "Iteration 296, loss = 0.09423019\n",
      "Iteration 297, loss = 0.09375948\n",
      "Iteration 298, loss = 0.09337557\n",
      "Iteration 299, loss = 0.09298133\n",
      "Iteration 300, loss = 0.09239909\n",
      "Iteration 301, loss = 0.09178049\n",
      "Iteration 302, loss = 0.09132333\n",
      "Iteration 303, loss = 0.09087253\n",
      "Iteration 304, loss = 0.09041146\n",
      "Iteration 305, loss = 0.08995299\n",
      "Iteration 306, loss = 0.08954190\n",
      "Iteration 307, loss = 0.08916394\n",
      "Iteration 308, loss = 0.08875899\n",
      "Iteration 309, loss = 0.08822596\n",
      "Iteration 310, loss = 0.08768244\n",
      "Iteration 311, loss = 0.08710955\n",
      "Iteration 312, loss = 0.08663207\n",
      "Iteration 313, loss = 0.08607996\n",
      "Iteration 314, loss = 0.08559621\n",
      "Iteration 315, loss = 0.08518660\n",
      "Iteration 316, loss = 0.08473322\n",
      "Iteration 317, loss = 0.08426287\n",
      "Iteration 318, loss = 0.08382979\n",
      "Iteration 319, loss = 0.08338998\n",
      "Iteration 320, loss = 0.08283261\n",
      "Iteration 321, loss = 0.08238218\n",
      "Iteration 322, loss = 0.08198482\n",
      "Iteration 323, loss = 0.08146850\n",
      "Iteration 324, loss = 0.08107097\n",
      "Iteration 325, loss = 0.08061374\n",
      "Iteration 326, loss = 0.08030027\n",
      "Iteration 327, loss = 0.07993425\n",
      "Iteration 328, loss = 0.07952288\n",
      "Iteration 329, loss = 0.07909790\n",
      "Iteration 330, loss = 0.07868079\n",
      "Iteration 331, loss = 0.07837296\n",
      "Iteration 332, loss = 0.07807380\n",
      "Iteration 333, loss = 0.07760366\n",
      "Iteration 334, loss = 0.07720487\n",
      "Iteration 335, loss = 0.07682917\n",
      "Iteration 336, loss = 0.07634554\n",
      "Iteration 337, loss = 0.07595850\n",
      "Iteration 338, loss = 0.07558941\n",
      "Iteration 339, loss = 0.07524884\n",
      "Iteration 340, loss = 0.07485372\n",
      "Iteration 341, loss = 0.07448966\n",
      "Iteration 342, loss = 0.07406757\n",
      "Iteration 343, loss = 0.07363728\n",
      "Iteration 344, loss = 0.07323336\n",
      "Iteration 345, loss = 0.07293929\n",
      "Iteration 346, loss = 0.07257627\n",
      "Iteration 347, loss = 0.07233773\n",
      "Iteration 348, loss = 0.07192418\n",
      "Iteration 349, loss = 0.07149262\n",
      "Iteration 350, loss = 0.07112139\n",
      "Iteration 351, loss = 0.07079588\n",
      "Iteration 352, loss = 0.07035476\n",
      "Iteration 353, loss = 0.06996045\n",
      "Iteration 354, loss = 0.06951757\n",
      "Iteration 355, loss = 0.06924322\n",
      "Iteration 356, loss = 0.06899342\n",
      "Iteration 357, loss = 0.06868992\n",
      "Iteration 358, loss = 0.06847642\n",
      "Iteration 359, loss = 0.06815675\n",
      "Iteration 360, loss = 0.06785303\n",
      "Iteration 361, loss = 0.06738082\n",
      "Iteration 362, loss = 0.06696605\n",
      "Iteration 363, loss = 0.06692805\n",
      "Iteration 364, loss = 0.06665303\n",
      "Iteration 365, loss = 0.06631380\n",
      "Iteration 366, loss = 0.06616902\n",
      "Iteration 367, loss = 0.06587637\n",
      "Iteration 368, loss = 0.06540474\n",
      "Iteration 369, loss = 0.06475809\n",
      "Iteration 370, loss = 0.06434064\n",
      "Iteration 371, loss = 0.06445375\n",
      "Iteration 372, loss = 0.06440478\n",
      "Iteration 373, loss = 0.06414404\n",
      "Iteration 374, loss = 0.06350785\n",
      "Iteration 375, loss = 0.06297191\n",
      "Iteration 376, loss = 0.06234285\n",
      "Iteration 377, loss = 0.06201839\n",
      "Iteration 378, loss = 0.06180772\n",
      "Iteration 379, loss = 0.06150055\n",
      "Iteration 380, loss = 0.06115502\n",
      "Iteration 381, loss = 0.06081999\n",
      "Iteration 382, loss = 0.06046550\n",
      "Iteration 383, loss = 0.06021334\n",
      "Iteration 384, loss = 0.05988466\n",
      "Iteration 385, loss = 0.05960736\n",
      "Iteration 386, loss = 0.05929283\n",
      "Iteration 387, loss = 0.05898432\n",
      "Iteration 388, loss = 0.05865565\n",
      "Iteration 389, loss = 0.05845299\n",
      "Iteration 390, loss = 0.05821231\n",
      "Iteration 391, loss = 0.05798064\n",
      "Iteration 392, loss = 0.05761638\n",
      "Iteration 393, loss = 0.05730622\n",
      "Iteration 394, loss = 0.05698324\n",
      "Iteration 395, loss = 0.05662468\n",
      "Iteration 396, loss = 0.05637650\n",
      "Iteration 397, loss = 0.05608215\n",
      "Iteration 398, loss = 0.05582686\n",
      "Iteration 399, loss = 0.05563357\n",
      "Iteration 400, loss = 0.05529529\n",
      "Iteration 401, loss = 0.05497162\n",
      "Iteration 402, loss = 0.05468560\n",
      "Iteration 403, loss = 0.05450352\n",
      "Iteration 404, loss = 0.05441684\n",
      "Iteration 405, loss = 0.05432442\n",
      "Iteration 406, loss = 0.05403535\n",
      "Iteration 407, loss = 0.05361497\n",
      "Iteration 408, loss = 0.05336037\n",
      "Iteration 409, loss = 0.05303458\n",
      "Iteration 410, loss = 0.05279851\n",
      "Iteration 411, loss = 0.05252648\n",
      "Iteration 412, loss = 0.05235135\n",
      "Iteration 413, loss = 0.05218578\n",
      "Iteration 414, loss = 0.05197826\n",
      "Iteration 415, loss = 0.05193365\n",
      "Iteration 416, loss = 0.05172315\n",
      "Iteration 417, loss = 0.05143756\n",
      "Iteration 418, loss = 0.05109274\n",
      "Iteration 419, loss = 0.05086613\n",
      "Iteration 420, loss = 0.05076892\n",
      "Iteration 421, loss = 0.05062168\n",
      "Iteration 422, loss = 0.05041323\n",
      "Iteration 423, loss = 0.05008582\n",
      "Iteration 424, loss = 0.04980758\n",
      "Iteration 425, loss = 0.04947318\n",
      "Iteration 426, loss = 0.04921529\n",
      "Iteration 427, loss = 0.04897102\n",
      "Iteration 428, loss = 0.04866910\n",
      "Iteration 429, loss = 0.04842694\n",
      "Iteration 430, loss = 0.04821401\n",
      "Iteration 431, loss = 0.04794448\n",
      "Iteration 432, loss = 0.04771276\n",
      "Iteration 433, loss = 0.04747341\n",
      "Iteration 434, loss = 0.04725383\n",
      "Iteration 435, loss = 0.04712909\n",
      "Iteration 436, loss = 0.04685876\n",
      "Iteration 437, loss = 0.04658346\n",
      "Iteration 438, loss = 0.04627621\n",
      "Iteration 439, loss = 0.04611995\n",
      "Iteration 440, loss = 0.04597074\n",
      "Iteration 441, loss = 0.04607355\n",
      "Iteration 442, loss = 0.04587115\n",
      "Iteration 443, loss = 0.04571263\n",
      "Iteration 444, loss = 0.04544811\n",
      "Iteration 445, loss = 0.04518050\n",
      "Iteration 446, loss = 0.04491188\n",
      "Iteration 447, loss = 0.04454910\n",
      "Iteration 448, loss = 0.04426720\n",
      "Iteration 449, loss = 0.04410016\n",
      "Iteration 450, loss = 0.04385608\n",
      "Iteration 451, loss = 0.04369580\n",
      "Iteration 452, loss = 0.04341231\n",
      "Iteration 453, loss = 0.04320788\n",
      "Iteration 454, loss = 0.04298909\n",
      "Iteration 455, loss = 0.04282326\n",
      "Iteration 456, loss = 0.04263573\n",
      "Iteration 457, loss = 0.04243037\n",
      "Iteration 458, loss = 0.04218534\n",
      "Iteration 459, loss = 0.04193019\n",
      "Iteration 460, loss = 0.04171544\n",
      "Iteration 461, loss = 0.04150831\n",
      "Iteration 462, loss = 0.04128540\n",
      "Iteration 463, loss = 0.04117871\n",
      "Iteration 464, loss = 0.04106583\n",
      "Iteration 465, loss = 0.04086161\n",
      "Iteration 466, loss = 0.04066957\n",
      "Iteration 467, loss = 0.04050690\n",
      "Iteration 468, loss = 0.04031674\n",
      "Iteration 469, loss = 0.04012864\n",
      "Iteration 470, loss = 0.03988403\n",
      "Iteration 471, loss = 0.03970602\n",
      "Iteration 472, loss = 0.03961484\n",
      "Iteration 473, loss = 0.03941772\n",
      "Iteration 474, loss = 0.03920624\n",
      "Iteration 475, loss = 0.03903336\n",
      "Iteration 476, loss = 0.03889294\n",
      "Iteration 477, loss = 0.03863314\n",
      "Iteration 478, loss = 0.03845332\n",
      "Iteration 479, loss = 0.03825973\n",
      "Iteration 480, loss = 0.03804515\n",
      "Iteration 481, loss = 0.03787851\n",
      "Iteration 482, loss = 0.03768820\n",
      "Iteration 483, loss = 0.03759818\n",
      "Iteration 484, loss = 0.03741026\n",
      "Iteration 485, loss = 0.03733939\n",
      "Iteration 486, loss = 0.03736127\n",
      "Iteration 487, loss = 0.03742036\n",
      "Iteration 488, loss = 0.03738921\n",
      "Iteration 489, loss = 0.03724759\n",
      "Iteration 490, loss = 0.03698307\n",
      "Iteration 491, loss = 0.03673293\n",
      "Iteration 492, loss = 0.03654928\n",
      "Iteration 493, loss = 0.03645276\n",
      "Iteration 494, loss = 0.03634632\n",
      "Iteration 495, loss = 0.03617832\n",
      "Iteration 496, loss = 0.03594847\n",
      "Iteration 497, loss = 0.03572506\n",
      "Iteration 498, loss = 0.03547997\n",
      "Iteration 499, loss = 0.03541492\n",
      "Iteration 500, loss = 0.03510205\n",
      "Iteration 501, loss = 0.03494455\n",
      "Iteration 502, loss = 0.03467867\n",
      "Iteration 503, loss = 0.03446471\n",
      "Iteration 504, loss = 0.03439704\n",
      "Iteration 505, loss = 0.03440183\n",
      "Iteration 506, loss = 0.03436178\n",
      "Iteration 507, loss = 0.03427617\n",
      "Iteration 508, loss = 0.03420284\n",
      "Iteration 509, loss = 0.03392218\n",
      "Iteration 510, loss = 0.03363006\n",
      "Iteration 511, loss = 0.03348108\n",
      "Iteration 512, loss = 0.03328957\n",
      "Iteration 513, loss = 0.03317092\n",
      "Iteration 514, loss = 0.03304733\n",
      "Iteration 515, loss = 0.03293473\n",
      "Iteration 516, loss = 0.03281744\n",
      "Iteration 517, loss = 0.03264089\n",
      "Iteration 518, loss = 0.03251148\n",
      "Iteration 519, loss = 0.03229629\n",
      "Iteration 520, loss = 0.03211419\n",
      "Iteration 521, loss = 0.03195656\n",
      "Iteration 522, loss = 0.03185236\n",
      "Iteration 523, loss = 0.03183128\n",
      "Iteration 524, loss = 0.03171144\n",
      "Iteration 525, loss = 0.03156778\n",
      "Iteration 526, loss = 0.03145891\n",
      "Iteration 527, loss = 0.03127828\n",
      "Iteration 528, loss = 0.03112181\n",
      "Iteration 529, loss = 0.03098346\n",
      "Iteration 530, loss = 0.03084917\n",
      "Iteration 531, loss = 0.03075080\n",
      "Iteration 532, loss = 0.03057551\n",
      "Iteration 533, loss = 0.03044380\n",
      "Iteration 534, loss = 0.03029432\n",
      "Iteration 535, loss = 0.03019264\n",
      "Iteration 536, loss = 0.03007657\n",
      "Iteration 537, loss = 0.02994927\n",
      "Iteration 538, loss = 0.02984479\n",
      "Iteration 539, loss = 0.02980732\n",
      "Iteration 540, loss = 0.02974834\n",
      "Iteration 541, loss = 0.02962908\n",
      "Iteration 542, loss = 0.02954429\n",
      "Iteration 543, loss = 0.02942954\n",
      "Iteration 544, loss = 0.02935956\n",
      "Iteration 545, loss = 0.02919643\n",
      "Iteration 546, loss = 0.02909940\n",
      "Iteration 547, loss = 0.02897653\n",
      "Iteration 548, loss = 0.02887197\n",
      "Iteration 549, loss = 0.02872034\n",
      "Iteration 550, loss = 0.02852378\n",
      "Iteration 551, loss = 0.02835194\n",
      "Iteration 552, loss = 0.02811119\n",
      "Iteration 553, loss = 0.02809315\n",
      "Iteration 554, loss = 0.02806396\n",
      "Iteration 555, loss = 0.02809019\n",
      "Iteration 556, loss = 0.02806386\n",
      "Iteration 557, loss = 0.02798654\n",
      "Iteration 558, loss = 0.02785619\n",
      "Iteration 559, loss = 0.02772272\n",
      "Iteration 560, loss = 0.02759064\n",
      "Iteration 561, loss = 0.02740992\n",
      "Iteration 562, loss = 0.02727679\n",
      "Iteration 563, loss = 0.02722755\n",
      "Iteration 564, loss = 0.02714722\n",
      "Iteration 565, loss = 0.02702392\n",
      "Iteration 566, loss = 0.02690747\n",
      "Iteration 567, loss = 0.02668073\n",
      "Iteration 568, loss = 0.02654000\n",
      "Iteration 569, loss = 0.02643958\n",
      "Iteration 570, loss = 0.02632343\n",
      "Iteration 571, loss = 0.02623108\n",
      "Iteration 572, loss = 0.02607649\n",
      "Iteration 573, loss = 0.02596483\n",
      "Iteration 574, loss = 0.02583375\n",
      "Iteration 575, loss = 0.02571824\n",
      "Iteration 576, loss = 0.02560827\n",
      "Iteration 577, loss = 0.02551835\n",
      "Iteration 578, loss = 0.02540573\n",
      "Iteration 579, loss = 0.02533733\n",
      "Iteration 580, loss = 0.02520465\n",
      "Iteration 581, loss = 0.02510530\n",
      "Iteration 582, loss = 0.02501297\n",
      "Iteration 583, loss = 0.02490946\n",
      "Iteration 584, loss = 0.02478738\n",
      "Iteration 585, loss = 0.02467576\n",
      "Iteration 586, loss = 0.02459991\n",
      "Iteration 587, loss = 0.02452038\n",
      "Iteration 588, loss = 0.02452428\n",
      "Iteration 589, loss = 0.02453417\n",
      "Iteration 590, loss = 0.02444270\n",
      "Iteration 591, loss = 0.02432495\n",
      "Iteration 592, loss = 0.02410462\n",
      "Iteration 593, loss = 0.02401046\n",
      "Iteration 594, loss = 0.02394431\n",
      "Iteration 595, loss = 0.02384529\n",
      "Iteration 596, loss = 0.02380027\n",
      "Iteration 597, loss = 0.02376456\n",
      "Iteration 598, loss = 0.02373864\n",
      "Iteration 599, loss = 0.02360471\n",
      "Iteration 600, loss = 0.02344565\n",
      "Iteration 601, loss = 0.02327607\n",
      "Iteration 602, loss = 0.02319387\n",
      "Iteration 603, loss = 0.02312452\n",
      "Iteration 604, loss = 0.02300912\n",
      "Iteration 605, loss = 0.02311255\n",
      "Iteration 606, loss = 0.02296549\n",
      "Iteration 607, loss = 0.02281105\n",
      "Iteration 608, loss = 0.02273240\n",
      "Iteration 609, loss = 0.02268374\n",
      "Iteration 610, loss = 0.02256096\n",
      "Iteration 611, loss = 0.02254282\n",
      "Iteration 612, loss = 0.02238205\n",
      "Iteration 613, loss = 0.02224875\n",
      "Iteration 614, loss = 0.02217811\n",
      "Iteration 615, loss = 0.02211745\n",
      "Iteration 616, loss = 0.02201373\n",
      "Iteration 617, loss = 0.02194113\n",
      "Iteration 618, loss = 0.02186035\n",
      "Iteration 619, loss = 0.02179309\n",
      "Iteration 620, loss = 0.02171084\n",
      "Iteration 621, loss = 0.02166879\n",
      "Iteration 622, loss = 0.02148764\n",
      "Iteration 623, loss = 0.02141160\n",
      "Iteration 624, loss = 0.02133580\n",
      "Iteration 625, loss = 0.02127671\n",
      "Iteration 626, loss = 0.02123948\n",
      "Iteration 627, loss = 0.02116757\n",
      "Iteration 628, loss = 0.02108930\n",
      "Iteration 629, loss = 0.02100971\n",
      "Iteration 630, loss = 0.02093986\n",
      "Iteration 631, loss = 0.02086835\n",
      "Iteration 632, loss = 0.02080600\n",
      "Iteration 633, loss = 0.02070503\n",
      "Iteration 634, loss = 0.02058407\n",
      "Iteration 635, loss = 0.02055440\n",
      "Iteration 636, loss = 0.02045471\n",
      "Iteration 637, loss = 0.02036749\n",
      "Iteration 638, loss = 0.02027998\n",
      "Iteration 639, loss = 0.02025777\n",
      "Iteration 640, loss = 0.02011683\n",
      "Iteration 641, loss = 0.02001344\n",
      "Iteration 642, loss = 0.01991555\n",
      "Iteration 643, loss = 0.01983617\n",
      "Iteration 644, loss = 0.01976815\n",
      "Iteration 645, loss = 0.01972379\n",
      "Iteration 646, loss = 0.01959956\n",
      "Iteration 647, loss = 0.01950311\n",
      "Iteration 648, loss = 0.01944761\n",
      "Iteration 649, loss = 0.01933310\n",
      "Iteration 650, loss = 0.01927826\n",
      "Iteration 651, loss = 0.01925038\n",
      "Iteration 652, loss = 0.01920328\n",
      "Iteration 653, loss = 0.01917139\n",
      "Iteration 654, loss = 0.01909528\n",
      "Iteration 655, loss = 0.01903073\n",
      "Iteration 656, loss = 0.01897376\n",
      "Iteration 657, loss = 0.01887855\n",
      "Iteration 658, loss = 0.01875671\n",
      "Iteration 659, loss = 0.01866746\n",
      "Iteration 660, loss = 0.01864546\n",
      "Iteration 661, loss = 0.01865902\n",
      "Iteration 662, loss = 0.01864572\n",
      "Iteration 663, loss = 0.01861953\n",
      "Iteration 664, loss = 0.01850819\n",
      "Iteration 665, loss = 0.01842694\n",
      "Iteration 666, loss = 0.01839904\n",
      "Iteration 667, loss = 0.01832669\n",
      "Iteration 668, loss = 0.01824066\n",
      "Iteration 669, loss = 0.01815490\n",
      "Iteration 670, loss = 0.01806452\n",
      "Iteration 671, loss = 0.01797429\n",
      "Iteration 672, loss = 0.01786121\n",
      "Iteration 673, loss = 0.01777894\n",
      "Iteration 674, loss = 0.01772085\n",
      "Iteration 675, loss = 0.01766644\n",
      "Iteration 676, loss = 0.01778193\n",
      "Iteration 677, loss = 0.01768557\n",
      "Iteration 678, loss = 0.01759579\n",
      "Iteration 679, loss = 0.01747220\n",
      "Iteration 680, loss = 0.01736672\n",
      "Iteration 681, loss = 0.01730152\n",
      "Iteration 682, loss = 0.01726633\n",
      "Iteration 683, loss = 0.01727777\n",
      "Iteration 684, loss = 0.01730345\n",
      "Iteration 685, loss = 0.01726185\n",
      "Iteration 686, loss = 0.01718709\n",
      "Iteration 687, loss = 0.01708463\n",
      "Iteration 688, loss = 0.01700384\n",
      "Iteration 689, loss = 0.01691901\n",
      "Iteration 690, loss = 0.01685874\n",
      "Iteration 691, loss = 0.01681078\n",
      "Iteration 692, loss = 0.01676076\n",
      "Iteration 693, loss = 0.01669455\n",
      "Iteration 694, loss = 0.01660781\n",
      "Iteration 695, loss = 0.01654353\n",
      "Iteration 696, loss = 0.01648775\n",
      "Iteration 697, loss = 0.01645305\n",
      "Iteration 698, loss = 0.01640869\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "clf = deepcopy(classifier)\n",
    "clf.fit(Xtrain_imputed, ytrain)\n",
    "pred = clf.predict(Xtest_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['knn', 'f1'] = f1_score(ytest, pred, average='macro')\n",
    "scores['knn', 'accuracy'] = accuracy_score(ytest, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.644723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.753762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missforest</th>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.644723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridge</th>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.811912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            accuracy        f1\n",
       "knn         0.716667  0.644723\n",
       "mean        0.800000  0.753762\n",
       "missforest  0.716667  0.644723\n",
       "ridge       0.850000  0.811912"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(scores).unstack()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
